{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, max_vocab_size=10000):\n",
    "        \"\"\"\n",
    "        Initialize a simple tokenizer with vocabulary building capabilities\n",
    "        \n",
    "        Args:\n",
    "            max_vocab_size (int): Maximum number of unique tokens to keep\n",
    "        \"\"\"\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.vocab_size = 0\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        \n",
    "        # Special tokens\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        self.sos_token = '<SOS>'  # Start of Sequence\n",
    "        self.eos_token = '<EOS>'  # End of Sequence\n",
    "        self._add_special_tokens()\n",
    "    \n",
    "    def _add_to_vocab(self, word):\n",
    "        \"\"\"Add a word to the vocabulary\"\"\"\n",
    "        if word not in self.word_to_index:\n",
    "            # If we still can add vocab\n",
    "            if self.vocab_size < self.max_vocab_size:\n",
    "                self.word_to_index[word] = self.vocab_size\n",
    "                self.index_to_word[self.vocab_size] = word\n",
    "                self.vocab_size += 1\n",
    "                # Return the latest index\n",
    "                return self.word_to_index[word]\n",
    "        # If the word already in the index of word if there \n",
    "        # OR max_vocab reached, return UNKNOWN Token\n",
    "        return self.word_to_index.get(word, self.word_to_index[self.unk_token])\n",
    "\n",
    "    def _add_special_tokens(self):\n",
    "        \"\"\"Add special tokens to the vocabulary\"\"\"\n",
    "        self._add_to_vocab(self.pad_token)\n",
    "        self._add_to_vocab(self.unk_token)\n",
    "        self._add_to_vocab(self.sos_token)\n",
    "        self._add_to_vocab(self.eos_token)\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        \"\"\"\n",
    "        Build vocabulary from input texts\n",
    "        \n",
    "        Args:\n",
    "            texts (list): List of sentences or documents\n",
    "        \"\"\"\n",
    "        # Flatten and tokenize texts; split by space for simplification\n",
    "        # each item is a word\n",
    "        all_words = [word for sentence in texts for word in sentence.split()]\n",
    "        \n",
    "        # Count word frequencies\n",
    "        # Ensures most common words are included first\n",
    "        word_counts = {}\n",
    "        for word in all_words:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        \n",
    "        # Sort words by frequency, descending order\n",
    "        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Add most frequent words to vocabulary\n",
    "        for word, _ in sorted_words:\n",
    "            self._add_to_vocab(word)\n",
    "\n",
    "    def texts_to_sequences(self, texts, add_sos_eos=True):\n",
    "        \"\"\"\n",
    "        Convert texts to sequences of indices\n",
    "        \n",
    "        Args:\n",
    "            texts (list): List of sentences or documents\n",
    "            add_sos_eos (bool): Whether to add Start and End of Sequence tokens\n",
    "        \n",
    "        Returns:\n",
    "            list: List of token indices\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            # Tokenize the text\n",
    "            tokens = text.split()\n",
    "            \n",
    "            # Add special tokens if requested\n",
    "            if add_sos_eos:\n",
    "                tokens = [self.sos_token] + tokens + [self.eos_token]\n",
    "            \n",
    "            # Convert to indices\n",
    "            sequence = [self.word_to_index.get(word, self.word_to_index[self.unk_token]) \n",
    "                        for word in tokens]\n",
    "            sequences.append(sequence)\n",
    "        \n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "        \"the quick brown fox jumps over the lazy dog\",\n",
    "        \"a quick brown fox jumps over a lazy dog\",\n",
    "        \"the lazy dog sleeps all day\"\n",
    "    ]\n",
    "    \n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit tokenizer on texts\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 4, 7, 8, 9, 10, 11, 4, 5, 6, 3], [2, 12, 7, 8, 9, 10, 11, 12, 5, 6, 3], [2, 4, 5, 6, 13, 14, 15, 3]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the sequences (3,11)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of the sequences ({len(sequences)},{len(sequences[0])})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
