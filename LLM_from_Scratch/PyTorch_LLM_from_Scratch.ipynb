{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Encoder-Decoder Small LLM with Hugging Face Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The device being used is `Window 11, Nvidia GPU 4070i 8GB`.\n",
    "\n",
    "`uv sync` in terminal to get all packages installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import List, Tuple, Dict, Any, Optional, Union\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.makedirs('models',exist_ok =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sample Data from Hugging Face\n",
    "\n",
    "We'll use the WikiText-2 dataset, a small and widely used dataset for language modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WikiText-2 dataset from Hugging Face\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Extract training and validation text data\n",
    "train_data = dataset[\"train\"][\"text\"]\n",
    "val_data = dataset[\"validation\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Data\n",
    "\n",
    "We need to tokenize the text data. We'll train a Byte Pair Encoding (BPE) tokenizer on the training data and use it to encode both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a BPE tokenizer\n",
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "# Use whitespace pre-tokenization to split on spaces\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Define special tokens for our model\n",
    "# <unk>: Unknown token for words not in vocabulary\n",
    "# <pad>: Padding token for batch processing\n",
    "# <bos>: Beginning of sequence token\n",
    "# <eos>: End of sequence token\n",
    "special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "\n",
    "# Train the tokenizer on the training data with a vocabulary size of 10,000\n",
    "trainer = BpeTrainer(vocab_size=10000, \n",
    "                     special_tokens=special_tokens)\n",
    "tokenizer.train_from_iterator(train_data, trainer)\n",
    "\n",
    "# Get token IDs for special tokens for later use\n",
    "pad_id = tokenizer.token_to_id(\"<pad>\")\n",
    "unk_id = tokenizer.token_to_id(\"<unk>\")\n",
    "bos_id = tokenizer.token_to_id(\"<bos>\")\n",
    "eos_id = tokenizer.token_to_id(\"<eos>\")\n",
    "\n",
    "# Encode the training and validation data\n",
    "train_encodings = tokenizer.encode_batch(train_data)\n",
    "val_encodings = tokenizer.encode_batch(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset Classes for Sequence-to-Sequence Tasks\n",
    "\n",
    "For an encoder-decoder model, we need input-output pairs. We'll use a sliding window approach to create these pairs from our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for sequence-to-sequence tasks that provides encoder input and decoder target pairs.\n",
    "    \n",
    "    Attributes:\n",
    "        encodings: List of tokenizer.Encoding objects containing token IDs\n",
    "        max_length: Maximum sequence length to consider (truncates longer sequences)\n",
    "        input_length: Length of the input sequence for the encoder\n",
    "        target_length: Length of the target sequence for the decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings: List, \n",
    "                 max_length: Optional[int] = None,\n",
    "                 input_length: int = 64, \n",
    "                 target_length: int = 64):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with encoded text.\n",
    "        \n",
    "        Args:\n",
    "            encodings: List of tokenizer.Encoding objects from the tokenizer\n",
    "            max_length: Maximum sequence length (optional)\n",
    "            input_length: Length of input sequences\n",
    "            target_length: Length of target sequences\n",
    "        \"\"\"\n",
    "        self.encodings = encodings\n",
    "        self.max_length = max_length\n",
    "        self.input_length = input_length\n",
    "        self.target_length = target_length\n",
    "        \n",
    "        # Filter out sequences that are too short\n",
    "        min_length = input_length + target_length\n",
    "        self.valid_indices = [i for i, enc in enumerate(encodings) \n",
    "                             if len(enc.ids) >= min_length]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of valid input-target pairs in the dataset.\"\"\"\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get an input-target pair by index.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the pair to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (input_tensor, target_tensor)\n",
    "        \"\"\"\n",
    "        # Get the real index from valid indices\n",
    "        real_idx = self.valid_indices[idx]\n",
    "        \n",
    "        # Get token IDs for the sequence\n",
    "        ids = self.encodings[real_idx].ids\n",
    "        \n",
    "        # Truncate if necessary\n",
    "        if self.max_length and len(ids) > self.max_length:\n",
    "            # Randomly select a starting point for the subsequence\n",
    "            max_start = len(ids) - self.max_length\n",
    "            start_idx = random.randint(0, max_start)\n",
    "            ids = ids[start_idx:start_idx + self.max_length]\n",
    "        \n",
    "        # For sequences longer than input_length + target_length, randomly select a split point\n",
    "        if len(ids) > self.input_length + self.target_length:\n",
    "            max_start = len(ids) - (self.input_length + self.target_length)\n",
    "            start_idx = random.randint(0, max_start)\n",
    "            ids = ids[start_idx:start_idx + self.input_length + self.target_length]\n",
    "        \n",
    "        # Split into input and target\n",
    "        input_ids = ids[:self.input_length]\n",
    "        target_ids = ids[self.input_length:self.input_length + self.target_length]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        input_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
    "        target_tensor = torch.tensor(target_ids, dtype=torch.long)\n",
    "            \n",
    "        return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function to handle variable length sequences in batches\n",
    "def collate_batch(batch: List[Tuple[torch.Tensor, torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Custom collate function for DataLoader that pads sequences to the same length.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of (input_tensor, target_tensor) pairs\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (padded_inputs, padded_targets)\n",
    "    \"\"\"\n",
    "    # Separate inputs and targets\n",
    "    inputs = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    \n",
    "    # Add BOS and EOS tokens to each sequence\n",
    "    inputs = [torch.cat([torch.tensor([bos_id]), seq, torch.tensor([eos_id])]) for seq in inputs]\n",
    "    targets = [torch.cat([torch.tensor([bos_id]), seq, torch.tensor([eos_id])]) for seq in targets]\n",
    "    \n",
    "    # Pad sequences to the same length\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=pad_id)\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=pad_id)\n",
    "    \n",
    "    return padded_inputs, padded_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sequences: 9173\n",
      "Number of validation sequences: 986\n"
     ]
    }
   ],
   "source": [
    "# Set sequence lengths for encoder-decoder model\n",
    "input_length = 64  # Length of input sequence for encoder\n",
    "target_length = 64  # Length of target sequence for decoder\n",
    "max_length = input_length + target_length  # Maximum total sequence length\n",
    "\n",
    "# Create datasets for training and validation\n",
    "train_dataset = Seq2SeqDataset(\n",
    "    train_encodings, \n",
    "    max_length=max_length, \n",
    "    input_length=input_length, \n",
    "    target_length=target_length\n",
    ")\n",
    "\n",
    "val_dataset = Seq2SeqDataset(\n",
    "    val_encodings, \n",
    "    max_length=max_length, \n",
    "    input_length=input_length, \n",
    "    target_length=target_length\n",
    ")\n",
    "\n",
    "print(f\"Number of training sequences: {len(train_dataset)}\")\n",
    "print(f\"Number of validation sequences: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Encoder-Decoder Model Architecture\n",
    "\n",
    "We'll define a transformer-based encoder-decoder model with separate encoder and decoder components, plus cross-attention to connect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderLLM(nn.Module):\n",
    "    \"\"\"\n",
    "    An encoder-decoder language model based on the transformer architecture.\n",
    "    \n",
    "    The model consists of an embedding layer, transformer encoder layers,\n",
    "    transformer decoder layers with cross-attention, and a final linear layer \n",
    "    that projects to vocabulary size for token prediction.\n",
    "    \n",
    "    Attributes:\n",
    "        embedding: Token embedding layer shared between encoder and decoder\n",
    "        encoder_pos_embedding: Positional encoding for encoder\n",
    "        decoder_pos_embedding: Positional encoding for decoder\n",
    "        encoder: Transformer encoder layers\n",
    "        decoder: Transformer decoder layers with cross-attention\n",
    "        fc: Final linear layer for token prediction\n",
    "        dropout: Dropout layer for regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 vocab_size: int, \n",
    "                 hidden_size: int, \n",
    "                 num_encoder_layers: int, \n",
    "                 num_decoder_layers: int,\n",
    "                 num_heads: int,\n",
    "                 max_seq_length: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize the encoder-decoder language model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of the vocabulary\n",
    "            hidden_size: Size of the hidden layers\n",
    "            num_encoder_layers: Number of transformer encoder layers\n",
    "            num_decoder_layers: Number of transformer decoder layers\n",
    "            num_heads: Number of attention heads in each transformer layer\n",
    "            max_seq_length: Maximum sequence length for positional embeddings\n",
    "            dropout: Dropout probability for regularization\n",
    "        \"\"\"\n",
    "        super(EncoderDecoderLLM, self).__init__()\n",
    "        \n",
    "        # Token embedding layer (shared between encoder and decoder)\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        \n",
    "        # Separate positional embeddings for encoder and decoder\n",
    "        self.encoder_pos_embedding = nn.Parameter(torch.zeros(1, max_seq_length, hidden_size))\n",
    "        self.decoder_pos_embedding = nn.Parameter(torch.zeros(1, max_seq_length, hidden_size))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size, \n",
    "            nhead=num_heads, \n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "        \n",
    "        # Transformer decoder with cross-attention to encoder outputs\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=hidden_size, \n",
    "            nhead=num_heads, \n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "        \n",
    "        # Final linear layer for token prediction\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize model weights for better training convergence.\n",
    "        \"\"\"\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder-decoder model.\n",
    "        \n",
    "        Args:\n",
    "            src: Input tensor for encoder with shape (batch_size, src_seq_length)\n",
    "            tgt: Input tensor for decoder with shape (batch_size, tgt_seq_length)\n",
    "            \n",
    "        Returns:\n",
    "            Logits tensor with shape (batch_size, tgt_seq_length, vocab_size)\n",
    "        \"\"\"\n",
    "        # Create attention masks for padding tokens\n",
    "        src_padding_mask = (src == pad_id)  # [batch_size, src_seq_len]\n",
    "        tgt_padding_mask = (tgt == pad_id)  # [batch_size, tgt_seq_len]\n",
    "        \n",
    "        # Create causal mask for decoder (to prevent looking at future tokens)\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n",
    "        \n",
    "        # Get sequence lengths\n",
    "        src_seq_len = src.size(1)  # Source sequence length\n",
    "        tgt_seq_len = tgt.size(1)  # Target sequence length\n",
    "        \n",
    "        # Embed tokens and add positional information\n",
    "        # [batch_size, src_seq_len, hidden_size]\n",
    "        src_emb = self.embedding(src) + self.encoder_pos_embedding[:, :src_seq_len, :]\n",
    "        \n",
    "        # [batch_size, tgt_seq_len, hidden_size]\n",
    "        tgt_emb = self.embedding(tgt) + self.decoder_pos_embedding[:, :tgt_seq_len, :]\n",
    "        \n",
    "        # Apply dropout\n",
    "        src_emb = self.dropout(src_emb)\n",
    "        tgt_emb = self.dropout(tgt_emb)\n",
    "        \n",
    "        # Encoder forward pass\n",
    "        # memory: [batch_size, src_seq_len, hidden_size]\n",
    "        memory = self.encoder(src_emb, src_key_padding_mask=src_padding_mask)\n",
    "        \n",
    "        # Decoder forward pass with cross-attention to encoder outputs\n",
    "        # output: [batch_size, tgt_seq_len, hidden_size]\n",
    "        output = self.decoder(\n",
    "            tgt_emb,                      # Input to decoder\n",
    "            memory,                       # Memory from encoder\n",
    "            tgt_mask=tgt_mask,           # Causal mask\n",
    "            memory_key_padding_mask=src_padding_mask,  # Encoder padding mask\n",
    "            tgt_key_padding_mask=tgt_padding_mask      # Decoder padding mask\n",
    "        )\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        # [batch_size, tgt_seq_len, vocab_size]\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def encode(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode the source sequence.\n",
    "        \n",
    "        Args:\n",
    "            src: Input tensor for encoder with shape (batch_size, src_seq_length)\n",
    "            \n",
    "        Returns:\n",
    "            Encoder output tensor with shape (batch_size, src_seq_length, hidden_size)\n",
    "        \"\"\"\n",
    "        # Create attention mask for padding tokens\n",
    "        src_padding_mask = (src == pad_id)\n",
    "        \n",
    "        # Get sequence length\n",
    "        src_seq_len = src.size(1)\n",
    "        \n",
    "        # Embed tokens and add positional information\n",
    "        src_emb = self.embedding(src) + self.encoder_pos_embedding[:, :src_seq_len, :]\n",
    "        src_emb = self.dropout(src_emb)\n",
    "        \n",
    "        # Encoder forward pass\n",
    "        memory = self.encoder(src_emb, src_key_padding_mask=src_padding_mask)\n",
    "        \n",
    "        return memory\n",
    "    \n",
    "    def decode(self, memory: torch.Tensor, tgt: torch.Tensor, \n",
    "               src_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decode with encoder memory and target sequence.\n",
    "        \n",
    "        Args:\n",
    "            memory: Encoder output with shape (batch_size, src_seq_length, hidden_size)\n",
    "            tgt: Input tensor for decoder with shape (batch_size, tgt_seq_length)\n",
    "            src_padding_mask: Optional mask for encoder padding \n",
    "            \n",
    "        Returns:\n",
    "            Logits tensor with shape (batch_size, tgt_seq_length, vocab_size)\n",
    "        \"\"\"\n",
    "        # Create attention masks\n",
    "        tgt_padding_mask = (tgt == pad_id)\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n",
    "        \n",
    "        # Embed tokens and add positional information\n",
    "        tgt_emb = self.embedding(tgt) + self.decoder_pos_embedding[:, :tgt_seq_len, :]\n",
    "        tgt_emb = self.dropout(tgt_emb)\n",
    "        \n",
    "        # Decoder forward pass\n",
    "        output = self.decoder(\n",
    "            tgt_emb, \n",
    "            memory, \n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10000\n",
      "Hidden size: 256\n",
      "Number of encoder layers: 3\n",
      "Number of decoder layers: 3\n",
      "Number of attention heads: 8\n",
      "Batch size: 32\n",
      "Learning rate: 0.0005\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "vocab_size = tokenizer.get_vocab_size()  # Size of the tokenizer's vocabulary\n",
    "hidden_size = 256  # Size of the hidden layers and embeddings\n",
    "num_encoder_layers = 3  # Number of encoder transformer layers \n",
    "num_decoder_layers = 3  # Number of decoder transformer layers\n",
    "num_heads = 8      # Number of attention heads\n",
    "max_seq_len = 128  # Maximum sequence length for the model\n",
    "\n",
    "# Training hyperparameters\n",
    "batch_size = 32    # Batch size for training\n",
    "learning_rate = 5e-4  # Learning rate for the optimizer\n",
    "num_epochs = 5     # Number of training epochs\n",
    "weight_decay = 0.01  # L2 regularization for the optimizer\n",
    "gradient_clipping = 1.0  # Gradient clipping value to prevent exploding gradients\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Hidden size: {hidden_size}\")\n",
    "print(f\"Number of encoder layers: {num_encoder_layers}\")\n",
    "print(f\"Number of decoder layers: {num_decoder_layers}\")\n",
    "print(f\"Number of attention heads: {num_heads}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model, Optimizer, and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hohoy\\OneDrive\\Desktop\\Small-LLM-PyTorch\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create the model instance\n",
    "model = EncoderDecoderLLM(\n",
    "    vocab_size=vocab_size, \n",
    "    hidden_size=hidden_size, \n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    num_heads=num_heads,\n",
    "    max_seq_length=max_seq_len,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Set up the AdamW optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# Create data loaders for training and validation with our custom collate function\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "# Learning rate scheduler for better convergence\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=1, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for Evaluation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(loss: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate perplexity from the loss value.\n",
    "    \n",
    "    Perplexity is a measure of how well a language model predicts a sample,\n",
    "    calculated as the exponential of the cross-entropy loss.\n",
    "    \n",
    "    Args:\n",
    "        loss: Cross-entropy loss value\n",
    "        \n",
    "    Returns:\n",
    "        Perplexity value\n",
    "    \"\"\"\n",
    "    return torch.exp(torch.tensor(loss)).item()\n",
    "\n",
    "def evaluate(model: nn.Module, \n",
    "             data_loader: DataLoader, \n",
    "             device: torch.device,\n",
    "             pad_id: int,\n",
    "             vocab_size: int) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the encoder-decoder model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        data_loader: DataLoader for the evaluation dataset\n",
    "        device: Device to use for computation\n",
    "        pad_id: ID of the padding token to ignore in loss calculation\n",
    "        vocab_size: Size of the vocabulary\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (average loss, perplexity)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        for src, tgt in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # Move batch to device\n",
    "            src = src.to(device)  # [batch_size, src_seq_len]\n",
    "            tgt = tgt.to(device)  # [batch_size, tgt_seq_len]\n",
    "            \n",
    "            # Get decoder input and target sequences\n",
    "            # Input: all tokens except the last one\n",
    "            decoder_input = tgt[:, :-1]  # [batch_size, tgt_seq_len-1]\n",
    "            # Target: all tokens except the first one\n",
    "            target_seq = tgt[:, 1:]    # [batch_size, tgt_seq_len-1]\n",
    "            \n",
    "            # Create mask to ignore padding tokens\n",
    "            padding_mask = (target_seq != pad_id)  # [batch_size, tgt_seq_len-1]\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src, decoder_input)  # [batch_size, tgt_seq_len-1, vocab_size]\n",
    "            \n",
    "            # Compute loss (ignoring padding tokens)\n",
    "            loss = F.cross_entropy(\n",
    "                output.reshape(-1, vocab_size),  # [batch_size*(tgt_seq_len-1), vocab_size]\n",
    "                target_seq.reshape(-1),         # [batch_size*(tgt_seq_len-1)]\n",
    "                ignore_index=pad_id,\n",
    "                reduction='sum'\n",
    "            )\n",
    "            \n",
    "            # Count non-padding tokens\n",
    "            num_tokens = padding_mask.sum().item()\n",
    "            \n",
    "            # Accumulate statistics\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += num_tokens\n",
    "    \n",
    "    # Calculate average loss and perplexity\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = calculate_perplexity(avg_loss)\n",
    "    \n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "We implement a training loop for the encoder-decoder model. The encoder processes the input sequence, and the decoder generates the target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d767df58e3b14345a774020d984b371a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hohoy\\OneDrive\\Desktop\\Small-LLM-PyTorch\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4d0c3b772e4bdfbb203aac8fb93a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hohoy\\OneDrive\\Desktop\\Small-LLM-PyTorch\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Time: 31.5s\n",
      "Train Loss: 6.7271 | Train Perplexity: 834.72\n",
      "Val Loss: 6.1930 | Val Perplexity: 489.31\n",
      "------------------------------------------------------------\n",
      "Saved best model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53049aac7c6430fb40703edf81be755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c565e5d3a54b11be9830f5fd52b073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | Time: 26.6s\n",
      "Train Loss: 5.9199 | Train Perplexity: 372.37\n",
      "Val Loss: 5.8354 | Val Perplexity: 342.19\n",
      "------------------------------------------------------------\n",
      "Saved best model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618db91e972f45b2b9f5d9443505979a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3465f99645f84006bb94118251f09ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | Time: 18.9s\n",
      "Train Loss: 5.5385 | Train Perplexity: 254.30\n",
      "Val Loss: 5.6402 | Val Perplexity: 281.52\n",
      "------------------------------------------------------------\n",
      "Saved best model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dfa2ee17301449c91a2749831def4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be71c9e2c14746e397cd912aa91f79d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | Time: 18.0s\n",
      "Train Loss: 5.2856 | Train Perplexity: 197.48\n",
      "Val Loss: 5.5315 | Val Perplexity: 252.51\n",
      "------------------------------------------------------------\n",
      "Saved best model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6b7a87268145bc8c91c5baec397560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6eaa9ade3047a5bd74219a86cd1d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | Time: 18.2s\n",
      "Train Loss: 5.0895 | Train Perplexity: 162.31\n",
      "Val Loss: 5.4357 | Val Perplexity: 229.46\n",
      "------------------------------------------------------------\n",
      "Saved best model!\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "training_stats = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    # Initialize progress bar for the training loop\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch_idx, (src, tgt) in enumerate(progress_bar):\n",
    "        # Move batch to device\n",
    "        src = src.to(device)  # [batch_size, src_seq_len]\n",
    "        tgt = tgt.to(device)  # [batch_size, tgt_seq_len]\n",
    "        \n",
    "        # Get decoder input and target sequences\n",
    "        # Decoder input: all tokens except the last one (shifted right)\n",
    "        decoder_input = tgt[:, :-1]  # [batch_size, tgt_seq_len-1]\n",
    "        # Target: all tokens except the first one\n",
    "        target_seq = tgt[:, 1:]    # [batch_size, tgt_seq_len-1]\n",
    "        \n",
    "        # Create mask to ignore padding tokens in the loss calculation\n",
    "        padding_mask = (target_seq != pad_id)  # [batch_size, tgt_seq_len-1]\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        \n",
    "        # output: [batch_size, tgt_seq_len-1, vocab_size]\n",
    "        output = model(src, decoder_input)\n",
    "        \n",
    "        # Compute loss (ignoring padding tokens)\n",
    "        loss = F.cross_entropy(\n",
    "            output.reshape(-1, vocab_size),  # [batch_size*(tgt_seq_len-1), vocab_size]\n",
    "            target_seq.reshape(-1),          # [batch_size*(tgt_seq_len-1)]\n",
    "            ignore_index=pad_id,            # Ignore padding tokens\n",
    "            reduction='sum'                 # Sum losses for proper token counting\n",
    "        )\n",
    "        \n",
    "        # Count non-padding tokens for proper loss scaling\n",
    "        num_tokens = padding_mask.sum().item()\n",
    "        \n",
    "        # Scale loss by number of tokens for better comparability\n",
    "        scaled_loss = loss / num_tokens\n",
    "        \n",
    "        # Backpropagation\n",
    "        scaled_loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "        \n",
    "        # Update model weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate statistics\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += num_tokens\n",
    "        \n",
    "        # Update progress bar with current loss\n",
    "        current_loss = loss.item() / num_tokens\n",
    "        current_perplexity = calculate_perplexity(current_loss)\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{current_loss:.4f}\",\n",
    "            'ppl': f\"{current_perplexity:.2f}\"\n",
    "        })\n",
    "    \n",
    "    # Calculate average loss and perplexity for the epoch\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = calculate_perplexity(avg_loss)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_perplexity = evaluate(model, val_loader, device, pad_id, vocab_size)\n",
    "    \n",
    "    # Adjust learning rate based on validation performance\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Calculate epoch time\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Print epoch statistics\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Time: {epoch_time:.1f}s\")\n",
    "    print(f\"Train Loss: {avg_loss:.4f} | Train Perplexity: {perplexity:.2f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Perplexity: {val_perplexity:.2f}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Save statistics\n",
    "    training_stats.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': avg_loss,\n",
    "        'train_perplexity': perplexity,\n",
    "        'val_loss': val_loss,\n",
    "        'val_perplexity': val_perplexity,\n",
    "        'epoch_time': epoch_time\n",
    "    })\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'models/best_encoder_decoder_model.pt')\n",
    "        print(\"Saved best model!\")\n",
    "\n",
    "# Load the best model for evaluation and generation\n",
    "model.load_state_dict(torch.load('models/best_encoder_decoder_model.pt'))\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "We evaluate the encoder-decoder model on the validation set by calculating perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af039d38c09d4a7983f0dfbaaeff4c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Loss: 5.4563\n",
      "Final Validation Perplexity: 234.22\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation on validation set\n",
    "val_loss, val_perplexity = evaluate(model, val_loader, device, pad_id, vocab_size)\n",
    "print(f\"Final Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Final Validation Perplexity: {val_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text with the Encoder-Decoder Model\n",
    "\n",
    "We use the trained encoder-decoder model to generate text from a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model: nn.Module, \n",
    "                  tokenizer: Tokenizer, \n",
    "                  prompt: str, \n",
    "                  max_length: int = 50, \n",
    "                  temperature: float = 1.0,\n",
    "                  top_k: Optional[int] = None,\n",
    "                  device: torch.device = device) -> str:\n",
    "    \"\"\"\n",
    "    Generate text continuation from a prompt using the trained encoder-decoder model.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained encoder-decoder model\n",
    "        tokenizer: Tokenizer used to encode/decode text\n",
    "        prompt: Starting text as input to the encoder\n",
    "        max_length: Maximum number of tokens to generate\n",
    "        temperature: Controls randomness (lower = more deterministic)\n",
    "        top_k: If set, sample from top k most likely tokens\n",
    "        device: Device to use for computation\n",
    "        \n",
    "    Returns:\n",
    "        Generated text string\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Encode the prompt for the encoder\n",
    "    encoder_input_ids = tokenizer.encode(prompt).ids\n",
    "    \n",
    "    # Add BOS token if not present\n",
    "    if encoder_input_ids[0] != bos_id:\n",
    "        encoder_input_ids = [bos_id] + encoder_input_ids\n",
    "    \n",
    "    # Add EOS token if not present\n",
    "    if encoder_input_ids[-1] != eos_id:\n",
    "        encoder_input_ids = encoder_input_ids + [eos_id]\n",
    "    \n",
    "    # Convert to tensor and move to device\n",
    "    encoder_input = torch.tensor([encoder_input_ids], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Start with just the BOS token for the decoder input\n",
    "    decoder_input = torch.tensor([[bos_id]], dtype=torch.long, device=device)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        # Encode the input sequence once (encoder forward pass)\n",
    "        memory = model.encode(encoder_input)\n",
    "        src_padding_mask = (encoder_input == pad_id)\n",
    "        \n",
    "        # Generate tokens one by one\n",
    "        for _ in range(max_length):\n",
    "            # Decode with the current decoder input\n",
    "            output = model.decode(memory, decoder_input, src_padding_mask)\n",
    "            \n",
    "            # Get logits for the next token (last position)\n",
    "            next_token_logits = output[:, -1, :] / temperature\n",
    "            \n",
    "            # Apply top-k sampling if specified\n",
    "            if top_k is not None:\n",
    "                # Get top k logits and their indices\n",
    "                top_k_logits, top_k_indices = torch.topk(next_token_logits, k=top_k, dim=-1)\n",
    "                \n",
    "                # Create a mask of the same shape as logits, filled with -inf\n",
    "                next_token_logits = torch.full_like(next_token_logits, float('-inf'))\n",
    "                \n",
    "                # Fill in the top-k logits\n",
    "                next_token_logits.scatter_(1, top_k_indices, top_k_logits)\n",
    "            \n",
    "            # Convert logits to probabilities using softmax\n",
    "            probabilities = F.softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            \n",
    "            # Stop if EOS token is generated\n",
    "            if next_token.item() == eos_id:\n",
    "                break\n",
    "                \n",
    "            # Add the next token to the decoder input\n",
    "            decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
    "    \n",
    "    # Get all generated tokens (excluding the initial BOS token)\n",
    "    generated_ids = decoder_input[0, 1:].tolist()  # Skip the first BOS token\n",
    "    \n",
    "    # Decode tokens back to text\n",
    "    generated_text = tokenizer.decode(generated_ids)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Once upon a time\n",
      "\n",
      "Temperature 0.7:\n",
      "his former team to support the team ' s assistant of all over two games . He was the first time since his first time because he had been in the season in the second season . On November 2010 , he was in the second time against his second\n",
      "\n",
      "Temperature 1.0:\n",
      "was un impressed with both his wife , he said that the time his first time he was at the final was the year as part of his first time . Two years later he wrote that he was an to make the victory his father , he finished ,\n",
      "\n",
      "Temperature 1.3:\n",
      "' s death , it was used in his debut , while his fourth final game at a second half @-@ season club , but would return after . On 4 he was then left in the record , he received him to get his wife ' s mother at\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt: The history of artificial intelligence\n",
      "\n",
      "Temperature 0.7:\n",
      "the film because he said that he was \" a little effect of the \" . He was a game of \" from \" I ' ve ' ll ' t know ingly all of my self that we ' ll just . \" The A ' s ' \"\n",
      "\n",
      "Temperature 1.0:\n",
      "a year @-@ game was later changed to his brother C ore and his time with his wife ' s father in his family and his brother . After stated that he did not have the only been critical of the most un ately , and the game ' s\n",
      "\n",
      "Temperature 1.3:\n",
      ". He also described that they could have been criticized from his time that of life , or in another episode is a game ' s name ' game because the band that the last time she was ' s poem . One , he is to play what is\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt: In recent years, researchers have\n",
      "\n",
      "Temperature 0.7:\n",
      "\" in the final day , the original video was also called the \" and a \" the book \" , \" The Good Ter ri el , \" , \" a \" . Some of the film was the last of the episode to be \" . The Best\n",
      "\n",
      "Temperature 1.0:\n",
      "in the country . The film was in both the poem , which was a small group of the first in which the same name for this \" . In the world of The Story of the same period of A ber ys was created by his own hand ,\n",
      "\n",
      "Temperature 1.3:\n",
      "w ished to the rest of the original \" . In March , the same time to see the whole of the same year @-@ F ab ir is to produce a number of three children before his father , he is about one of two years , not that\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate text with different prompts and parameters\n",
    "test_prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The history of artificial intelligence\",\n",
    "    \"In recent years, researchers have\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    \n",
    "    # Generate with different temperatures\n",
    "    for temp in [0.7, 1.0, 1.3]:\n",
    "        generated = generate_text(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            max_length=50,\n",
    "            temperature=temp,\n",
    "            top_k=40\n",
    "        )\n",
    "        print(f\"\\nTemperature {temp}:\\n{generated}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the model weights\n",
    "torch.save(model.state_dict(), 'models/encoder_decoder_llm_model.pt')\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"models/tokenizer.json\")\n",
    "\n",
    "print(\"Model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path: str, \n",
    "                             tokenizer_path: str, \n",
    "                             device: torch.device = torch.device(\"cuda\")) -> Tuple[nn.Module, Tokenizer]:\n",
    "    \"\"\"\n",
    "    Load a saved encoder-decoder model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model weights\n",
    "        tokenizer_path: Path to the saved tokenizer\n",
    "        device: Device to load the model on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (loaded model, loaded tokenizer)\n",
    "    \"\"\"\n",
    "    # Load tokenizer\n",
    "    tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "    \n",
    "    # Get vocabulary size\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    \n",
    "    # Create model instance with the same architecture\n",
    "    model = EncoderDecoderLLM(\n",
    "        vocab_size=vocab_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        num_decoder_layers=num_decoder_layers,\n",
    "        num_heads=num_heads,\n",
    "        max_seq_length=max_seq_len\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load model weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated with loaded model:\n",
      "Mac Donald did not been erected in southeastern one as the world as well as . A Lo oking cultural tend ency also move as E ps of Saint scholar Henry M au is currently in the 2014 S of He also released viewed the UK . A ction compl\n"
     ]
    }
   ],
   "source": [
    "# Test loading the model and tokenizer\n",
    "loaded_model, loaded_tokenizer = load_model_and_tokenizer(\n",
    "    model_path=\"models/encoder_decoder_llm_model.pt\",\n",
    "    tokenizer_path=\"models/tokenizer.json\"\n",
    ")\n",
    "\n",
    "# Generate text with the loaded model\n",
    "test_prompt = \"The future of technology is\"\n",
    "generated_text = generate_text(\n",
    "    model=loaded_model,\n",
    "    tokenizer=loaded_tokenizer,\n",
    "    prompt=test_prompt,\n",
    "    max_length=50\n",
    ")\n",
    "\n",
    "print(f\"Generated with loaded model:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've built a small-scale encoder-decoder language model using the transformer architecture. Here's a summary of what we've accomplished:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - Loaded the WikiText-2 dataset\n",
    "   - Trained a BPE tokenizer to convert text to token IDs\n",
    "   - Created custom seq2seq dataset and dataloader with padding to handle variable sequence lengths\n",
    "\n",
    "2. **Encoder-Decoder Architecture**:\n",
    "   - Implemented a complete transformer-based encoder-decoder model\n",
    "   - Added cross-attention in the decoder to attend to encoder outputs\n",
    "   - Used separate positional embeddings for encoder and decoder\n",
    "   - Implemented causal masking in the decoder to prevent looking at future tokens\n",
    "\n",
    "3. **Training**:\n",
    "   - Trained the model using teacher forcing for the decoder\n",
    "   - Used learning rate scheduling to improve convergence\n",
    "   - Implemented gradient clipping to prevent exploding gradients\n",
    "   - Saved the best model based on validation performance\n",
    "\n",
    "4. **Evaluation and Generation**:\n",
    "   - Evaluated model using perplexity, a standard language modeling metric\n",
    "   - Implemented text generation with autoregressive decoding\n",
    "   - Added temperature and top-k sampling for controlling generation diversity\n",
    "   - Added functions to save and load the model for future use\n",
    "\n",
    "This encoder-decoder architecture is more flexible than the original encoder-only model and is better suited for text generation tasks. The encoder processes the input context, and the decoder generates new text conditioned on that context. This approach is similar to what's used in larger language models (LLMs) like T5, BART, and other sequence-to-sequence transformer models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
