{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences shape: torch.Size([3, 8])\n",
      "Vocab size: 14\n",
      "\n",
      "Reconstructed texts:\n",
      "Original:     hello world\n",
      "Reconstructed: hello world\n",
      "\n",
      "Original:     machine learning is awesome\n",
      "Reconstructed: machine learning is awesome\n",
      "\n",
      "Original:     pytorch is great for deep learning\n",
      "Reconstructed: pytorch is great for deep learning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Union, Optional, Any\n",
    "from torch import Tensor\n",
    "import math\n",
    "class PyTorchTokenizer(nn.Module):\n",
    "    def __init__(self, max_vocab_size: int = 10000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Special token indices\n",
    "        self.PAD_IDX: int = 0\n",
    "        self.UNK_IDX: int = 1\n",
    "        self.SOS_IDX: int = 2\n",
    "        self.EOS_IDX: int = 3\n",
    "        \n",
    "        # Tokenizer attributes\n",
    "        self.max_vocab_size: int = max_vocab_size\n",
    "        self.word_to_index: Dict[str, int] = {}\n",
    "        self.index_to_word: Dict[int, str] = {}\n",
    "        \n",
    "        # Special tokens\n",
    "        special_tokens: List[str] = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n",
    "        for idx, token in enumerate(special_tokens):\n",
    "            self.word_to_index[token] = idx\n",
    "            self.index_to_word[idx] = token\n",
    "        \n",
    "        # Track vocabulary size\n",
    "        self.vocab_size: int = len(special_tokens)\n",
    "    \n",
    "    def fit_on_texts(self, texts: List[str]) -> None:\n",
    "        \"\"\"Build vocabulary from input texts\"\"\"\n",
    "        # Tokenize and count word frequencies\n",
    "        words: List[str] = [word for text in texts for word in text.split()]\n",
    "        word_counts: Counter = Counter(words)\n",
    "        \n",
    "        # Sort words by frequency, descending order\n",
    "        sorted_words: List[tuple] = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Add most frequent words to vocabulary\n",
    "        for word, _ in sorted_words:\n",
    "            if word not in self.word_to_index and self.vocab_size < self.max_vocab_size:\n",
    "                self.word_to_index[word] = self.vocab_size\n",
    "                self.index_to_word[self.vocab_size] = word\n",
    "                self.vocab_size += 1\n",
    "    \n",
    "    def texts_to_sequences(\n",
    "        self, \n",
    "        texts: List[str], \n",
    "        add_sos_eos: bool = True, \n",
    "        max_length: Optional[int] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert texts to tensor or list of indices\n",
    "        \n",
    "        Args:\n",
    "            texts: List of sentences\n",
    "            add_sos_eos: Add start/end tokens\n",
    "            padding: Pad sequences to same length\n",
    "            max_length: Maximum sequence length\n",
    "        \n",
    "        Returns:\n",
    "            Padded sequence tensor or list of sequences\n",
    "        \"\"\"\n",
    "        # Convert to indices\n",
    "        sequences: List[List[int]] = []\n",
    "        for text in texts:\n",
    "            tokens: List[str] = text.split()\n",
    "            \n",
    "            # Add special tokens if requested\n",
    "            if add_sos_eos:\n",
    "                tokens = ['<SOS>'] + tokens + ['<EOS>']\n",
    "            \n",
    "            # Convert to indices\n",
    "            sequence: List[int] = [self.word_to_index.get(word, self.UNK_IDX) for word in tokens]\n",
    "            sequences.append(sequence)\n",
    "        \n",
    "        # Determine max length\n",
    "        if max_length is None:\n",
    "            max_length = max(len(seq) for seq in sequences)\n",
    "        \n",
    "        # Pad sequences\n",
    "        \n",
    "        padded_sequences: List[List[int]] = []\n",
    "        for seq in sequences:\n",
    "            # Truncate or pad\n",
    "            seq = seq[:max_length]\n",
    "            seq = seq + [self.PAD_IDX] * (max_length - len(seq))\n",
    "            padded_sequences.append(seq)\n",
    "        \n",
    "        return torch.tensor(padded_sequences, dtype=torch.long)\n",
    "\n",
    "    \n",
    "    def sequences_to_texts(self, sequences: torch.Tensor) -> List[str]:\n",
    "        \"\"\"Convert sequences back to texts\"\"\"\n",
    "        texts: List[str] = []\n",
    "        for sequence in sequences:\n",
    "            # Convert indices to words; .item to get from torch\n",
    "            words: List[str] = [self.index_to_word.get(idx.item(), '<UNK>') for idx in sequence]\n",
    "            # Remove special tokens and padding\n",
    "            words = [w for w in words if w not in ['<PAD>', '<SOS>', '<EOS>']]\n",
    "            texts.append(' '.join(words))\n",
    "        return texts\n",
    "\n",
    "    # Sample texts\n",
    "texts: List[str] = [\n",
    "    \"hello world\",\n",
    "    \"machine learning is awesome\",\n",
    "    \"pytorch is great for deep learning\"\n",
    "]\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer: PyTorchTokenizer = PyTorchTokenizer(max_vocab_size=20)\n",
    "\n",
    "# Fit on texts\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert to sequences\n",
    "sequences: torch.Tensor = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "print(\"Sequences shape:\", sequences.shape)\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "\n",
    "# Convert back to texts\n",
    "reconstructed_texts: List[str] = tokenizer.sequences_to_texts(sequences)\n",
    "print(\"\\nReconstructed texts:\")\n",
    "for original, reconstructed in zip(texts, reconstructed_texts):\n",
    "    print(f\"Original:     {original}\")\n",
    "    print(f\"Reconstructed: {reconstructed}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([3, 8, 6])\n",
      "There are 3 sentences/batch size; 8 unique tokens; 6 d_model\n"
     ]
    }
   ],
   "source": [
    "class PyTorchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        token_size: int, \n",
    "        d_model: int, \n",
    "        padding_idx: Optional[int] = None,\n",
    "        init_method: str = 'uniform'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Custom embedding layer without using nn.Embedding\n",
    "        \n",
    "        Args:\n",
    "            token_size: Number of tokens in vocabulary\n",
    "            d_model: Embedding dimension\n",
    "            padding_idx: Index to set to zero\n",
    "            init_method: Weight initialization method\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Initialize weights based on method\n",
    "        if init_method == 'uniform':\n",
    "            self.weights = torch.rand(token_size, d_model) * 2 - 1  # [-1, 1]\n",
    "        elif init_method == 'normal':\n",
    "            self.weights = torch.randn(token_size, d_model)\n",
    "        elif init_method == 'xavier':\n",
    "            self.weights = torch.nn.init.xavier_uniform_(\n",
    "                torch.empty(token_size, d_model)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown init method: {init_method}\")\n",
    "        \n",
    "        # Zero out padding index if specified\n",
    "        if padding_idx is not None:\n",
    "            self.weights[padding_idx].zero_()\n",
    "        \n",
    "        self.token_size = token_size\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, token_sequences: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Lookup embeddings for given indices\n",
    "        \n",
    "        Args:\n",
    "            token_sequences: Tensor of token indices (# sentence, token_size)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of embedded tokens\n",
    "        \"\"\"\n",
    "        # Create output tensor\n",
    "        output = torch.zeros(\n",
    "            token_sequences.shape[0],  # batch size\n",
    "            token_sequences.shape[1],  # sequence length \n",
    "            self.d_model,      # embedding dimension\n",
    "            dtype=self.weights.dtype\n",
    "        )\n",
    "        print(f'Output shape: {output.shape}')\n",
    "        \n",
    "        # Manually lookup embeddings\n",
    "        for i, sentence in enumerate(token_sequences):\n",
    "            for j, token_idx in enumerate(sentence):\n",
    "                output[i, j] = self.weights[token_idx]\n",
    "        \n",
    "        return output\n",
    "    def __call__(self, indices: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Make the class callable for convenience\n",
    "        \"\"\"\n",
    "        return self.forward(indices)\n",
    "embedding = PyTorchEmbedding(token_size=20,\n",
    "                             d_model=6)\n",
    "outputs = embedding.forward(sequences)\n",
    "print((f'There are {outputs.shape[0]} sentences/batch size;'),\n",
    "    (f\"{outputs.shape[1]} unique tokens;\"),\n",
    "    f\"{outputs.shape[2]} d_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchSelfAttention(nn.Module):\n",
    "    def __init__(self, num_head: int = 8, d_model: int = 64):\n",
    "        super().__init__()\n",
    "        assert (d_model % num_head == 0), 'd_model must be divisible by num_head'\n",
    "        \n",
    "        self.num_head = num_head\n",
    "        self.d_model = d_model\n",
    "        self.d_head = int(d_model / num_head)\n",
    "        self.W_q = nn.Parameter(torch.randn(d_model, d_model))\n",
    "        self.W_v = nn.Parameter(torch.randn(d_model, d_model))\n",
    "        self.W_k = nn.Parameter(torch.randn(d_model, d_model))\n",
    "        self.W_o = nn.Parameter(torch.randn(d_model, d_model))\n",
    "\n",
    "    def split_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        return x.reshape(batch_size, self.num_head, seq_len, self.d_head)\n",
    "    \n",
    "    def join_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[2]\n",
    "        return x.reshape(batch_size, seq_len, self.d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        q = torch.matmul(x, self.W_q)\n",
    "        v = torch.matmul(x, self.W_v)\n",
    "        k = torch.matmul(x, self.W_k)\n",
    "        \n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "\n",
    "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        head_output = torch.matmul(attention_weights, v)\n",
    "        \n",
    "        output = self.join_heads(head_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Config: batch_size=2, seq_len=10, d_model=64\n",
      "Input shape: torch.Size([2, 10, 64])\n",
      "Q after split shape: torch.Size([2, 8, 10, 8])\n",
      "Output shape: torch.Size([2, 10, 64])\n",
      "\n",
      "Config: batch_size=4, seq_len=20, d_model=128\n",
      "Input shape: torch.Size([4, 20, 128])\n",
      "Q after split shape: torch.Size([4, 8, 20, 16])\n",
      "Output shape: torch.Size([4, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "test_configs = [\n",
    "        (2, 10, 64),   # batch_size=2, seq_len=10, d_model=64\n",
    "        (4, 20, 128),  # batch_size=4, seq_len=20, d_model=128\n",
    "    ]\n",
    "\n",
    "for batch_size, seq_len, d_model in test_configs:\n",
    "    # Create multi-head attention with default 8 heads\n",
    "    sa = PyTorchSelfAttention(num_head=8, d_model=d_model)\n",
    "    \n",
    "    # Create zero tensor input\n",
    "    x = torch.zeros(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Run forward pass\n",
    "    output = sa(x)\n",
    "    \n",
    "    print(f\"\\nConfig: batch_size={batch_size}, seq_len={seq_len}, d_model={d_model}\")\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    \n",
    "    # Verify intermediate shapes\n",
    "    q = torch.matmul(x, sa.W_q)\n",
    "    q_split = sa.split_heads(q)\n",
    "    print(\"Q after split shape:\", q_split.shape)\n",
    "    \n",
    "    # Verifying output shape matches input shape\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    assert output.shape == x.shape, \"Output shape must match input shape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
