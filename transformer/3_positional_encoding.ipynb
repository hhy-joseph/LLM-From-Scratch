{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings with Positional Encoding for Transformers\n",
    "\n",
    "This notebook builds on our previous work with custom tokenizers and embeddings to implement positional encodings as used in transformer models. Unlike RNNs, transformers process all tokens in parallel and have no inherent understanding of sequence order. Positional encodings solve this by explicitly adding position information to token embeddings.\n",
    "\n",
    "We'll cover:\n",
    "1. **Token Embeddings**: Review from previous notebook\n",
    "2. **Positional Encodings**: Implementation of sinusoidal position representations\n",
    "3. **Embedding with Position**: Combining token embeddings with positional information\n",
    "4. **Visualization**: Understanding how positional encodings work\n",
    "5. **Practical Usage**: Applying these components in a transformer context\n",
    "\n",
    "Let's first import the necessary libraries and our previously implemented components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union, Tuple, Optional, Any\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Import our custom tokenizer and embedding from previous notebook\n",
    "from utils.tokenizer import Tokenizer\n",
    "from utils.embedding import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Positional Encoding Implementation\n",
    "\n",
    "In the original \"Attention is All You Need\" paper, the authors used sinusoidal functions to create positional encodings. The key benefits of this approach are:\n",
    "\n",
    "1. It can handle sequences of arbitrary length, even those longer than seen during training\n",
    "2. It creates a unique pattern for each position\n",
    "3. The relative positions have a consistent relationship in the encoding space\n",
    "\n",
    "The formula for position encoding is:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$\n",
    "\n",
    "Where:\n",
    "- $pos$ is the position in the sequence\n",
    "- $i$ is the dimension index\n",
    "- $d_{model}$ is the embedding dimension\n",
    "\n",
    "Let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for tokenizer\n",
    "texts = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"a quick brown fox jumps over a lazy dog\",\n",
    "    \"the lazy dog sleeps all day\"\n",
    "]\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit tokenizer on texts\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "embedding_dim = 5  # Small dimension for demonstration\n",
    "embedding = Embedding(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    padding_idx=tokenizer.word_to_index[tokenizer.pad_token]\n",
    ")\n",
    "\n",
    "# Get embedding for a single token\n",
    "token_idx = tokenizer.word_to_index.get(\"the\", tokenizer.word_to_index[tokenizer.unk_token])\n",
    "token_embedding = embedding(token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_idxs = [tokenizer.word_to_index.get(word,\\\n",
    "                    tokenizer.word_to_index[tokenizer.unk_token]) \n",
    "              for word in texts[0].split()]\n",
    "token_embeddings = [embedding(token_idx) for token_idx in token_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of senetence one in texts is (9,5)\n"
     ]
    }
   ],
   "source": [
    "# 9 words/9 tokens with 5 embedding dimension\n",
    "print(f'Dimension of senetence one in texts is \\\n",
    "({len(token_embeddings)},{len(token_embeddings[0])})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding positional encoding\n",
    "\n",
    "Since we have 5 dimensions in the embedding, so we need 5 positional encoding.\n",
    "\n",
    "The standard sinusoidal positional encoding from the `\"Attention is All You Need\"` paper uses a specific pattern of sine and cosine functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(seq_length: int, d_model: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create sinusoidal positional encodings for sequences.\n",
    "    \n",
    "    Args:\n",
    "        seq_length: Length of the sequence\n",
    "        d_model: Dimensionality of the embeddings\n",
    "        \n",
    "    Returns:\n",
    "        Positional encoding matrix of shape (seq_length, d_model)\n",
    "    \"\"\"\n",
    "    # Position: # unique tokens\n",
    "    position: np.ndarray = np.arange(seq_length)[:, np.newaxis]\n",
    "    \n",
    "    div_term: np.ndarray = np.exp(np.arange(0, d_model, 1) * -(np.log(10000.0) / d_model))\n",
    "\n",
    "    pos_encoding: np.ndarray = np.zeros((seq_length, d_model))\n",
    "    print(f'The shape of the positional encoding is {pos_encoding.shape}')\n",
    "    pos_encoding[:, 0::2] = np.sin(position * div_term)[:, 0::2]\n",
    "    pos_encoding[:, 1::2] = np.cos(position * div_term)[:, 1::2]\n",
    "    \n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the positional encoding is (9, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get token embeddings for each token in the sequence\n",
    "token_idxs: List[int] = [tokenizer.word_to_index.get(word, tokenizer.word_to_index[tokenizer.unk_token]) \n",
    "                        for word in texts[0].split()]\n",
    "token_embeddings: np.ndarray = np.array([embedding(token_idx) for token_idx in token_idxs])\n",
    "\n",
    "# Generate positional encodings, aka noumber of tokens\n",
    "seq_length: int = len(token_embeddings)\n",
    "pos_encodings: np.ndarray = get_positional_encoding(seq_length, embedding_dim)\n",
    "\n",
    "# Add positional encodings to token embeddings\n",
    "token_pos_embeddings: np.ndarray = token_embeddings + pos_encodings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
