{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Transformer from Scratch with PyTorch\n",
    "\n",
    "In this notebook, I'll implement a transformer architecture using PyTorch. Instead of using pre-defined components, I'll build each part from scratch, including:\n",
    "\n",
    "- Custom tokenizer\n",
    "- Embedding layer\n",
    "- Positional encoding\n",
    "- Self-attention mechanism\n",
    "- Encoder and decoder layers\n",
    "- Complete transformer model\n",
    "\n",
    "This approach helps to understand the inner workings of transformer models that power modern NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences shape: torch.Size([3, 8])\n",
      "Vocab size: 14\n",
      "\n",
      "Reconstructed texts:\n",
      "Original:     hello world\n",
      "Reconstructed: hello world\n",
      "\n",
      "Original:     machine learning is awesome\n",
      "Reconstructed: machine learning is awesome\n",
      "\n",
      "Original:     pytorch is great for deep learning\n",
      "Reconstructed: pytorch is great for deep learning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Union, Optional, Any\n",
    "from torch import Tensor\n",
    "import math\n",
    "class PyTorchTokenizer(nn.Module):\n",
    "    def __init__(self, max_vocab_size: int = 10000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Special token indices\n",
    "        self.PAD_IDX: int = 0\n",
    "        self.UNK_IDX: int = 1\n",
    "        self.SOS_IDX: int = 2\n",
    "        self.EOS_IDX: int = 3\n",
    "        \n",
    "        # Tokenizer attributes\n",
    "        self.max_vocab_size: int = max_vocab_size\n",
    "        self.word_to_index: Dict[str, int] = {}\n",
    "        self.index_to_word: Dict[int, str] = {}\n",
    "        \n",
    "        # Special tokens\n",
    "        special_tokens: List[str] = ['<PAD>', '<UNK>', '< SOS >', '<EOS>']\n",
    "        for idx, token in enumerate(special_tokens):\n",
    "            self.word_to_index[token] = idx\n",
    "            self.index_to_word[idx] = token\n",
    "        \n",
    "        # Track vocabulary size\n",
    "        self.vocab_size: int = len(special_tokens)\n",
    "    \n",
    "    def fit_on_texts(self, texts: List[str]) -> None:\n",
    "        \"\"\"Build vocabulary from input texts\"\"\"\n",
    "        # Tokenize and count word frequencies\n",
    "        words: List[str] = [word for text in texts for word in text.split()]\n",
    "        word_counts: Counter = Counter(words)\n",
    "        \n",
    "        # Sort words by frequency, descending order\n",
    "        sorted_words: List[tuple] = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Add most frequent words to vocabulary\n",
    "        for word, _ in sorted_words:\n",
    "            if word not in self.word_to_index and self.vocab_size < self.max_vocab_size:\n",
    "                self.word_to_index[word] = self.vocab_size\n",
    "                self.index_to_word[self.vocab_size] = word\n",
    "                self.vocab_size += 1\n",
    "    \n",
    "    def texts_to_sequences(\n",
    "        self, \n",
    "        texts: List[str], \n",
    "        add_sos_eos: bool = True, \n",
    "        max_length: Optional[int] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert texts to tensor or list of indices\n",
    "        \n",
    "        Args:\n",
    "            texts: List of sentences\n",
    "            add_sos_eos: Add start/end tokens\n",
    "            padding: Pad sequences to same length\n",
    "            max_length: Maximum sequence length\n",
    "        \n",
    "        Returns:\n",
    "            Padded sequence tensor or list of sequences\n",
    "        \"\"\"\n",
    "        # Convert to indices\n",
    "        sequences: List[List[int]] = []\n",
    "        for text in texts:\n",
    "            tokens: List[str] = text.split()\n",
    "            \n",
    "            # Add special tokens if requested\n",
    "            if add_sos_eos:\n",
    "                tokens = ['< SOS >'] + tokens + ['<EOS>']\n",
    "            \n",
    "            # Convert to indices\n",
    "            sequence: List[int] = [self.word_to_index.get(word, self.UNK_IDX) for word in tokens]\n",
    "            sequences.append(sequence)\n",
    "        \n",
    "        # Determine max length\n",
    "        if max_length is None:\n",
    "            max_length = max(len(seq) for seq in sequences)\n",
    "        \n",
    "        # Pad sequences\n",
    "        \n",
    "        padded_sequences: List[List[int]] = []\n",
    "        for seq in sequences:\n",
    "            # Truncate or pad\n",
    "            seq = seq[:max_length]\n",
    "            seq = seq + [self.PAD_IDX] * (max_length - len(seq))\n",
    "            padded_sequences.append(seq)\n",
    "        \n",
    "        return torch.tensor(padded_sequences, dtype=torch.long)\n",
    "\n",
    "    \n",
    "    def sequences_to_texts(self, sequences: torch.Tensor) -> List[str]:\n",
    "        \"\"\"Convert sequences back to texts\"\"\"\n",
    "        texts: List[str] = []\n",
    "        for sequence in sequences:\n",
    "            # Convert indices to words; .item to get from torch\n",
    "            words: List[str] = [self.index_to_word.get(idx.item(), '<UNK>') for idx in sequence]\n",
    "            # Remove special tokens and padding\n",
    "            words = [w for w in words if w not in ['<PAD>', '< SOS >', '<EOS>']]\n",
    "            texts.append(' '.join(words))\n",
    "        return texts\n",
    "\n",
    "    # Sample texts\n",
    "texts: List[str] = [\n",
    "    \"hello world\",\n",
    "    \"machine learning is awesome\",\n",
    "    \"pytorch is great for deep learning\"\n",
    "]\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer: PyTorchTokenizer = PyTorchTokenizer(max_vocab_size=20)\n",
    "\n",
    "# Fit on texts\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert to sequences\n",
    "sequences: torch.Tensor = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "print(\"Sequences shape:\", sequences.shape)\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "\n",
    "# Convert back to texts\n",
    "reconstructed_texts: List[str] = tokenizer.sequences_to_texts(sequences)\n",
    "print(\"\\nReconstructed texts:\")\n",
    "for original, reconstructed in zip(texts, reconstructed_texts):\n",
    "    print(f\"Original:     {original}\")\n",
    "    print(f\"Reconstructed: {reconstructed}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Embedding Layer\n",
    "\n",
    "This class implements a custom embedding layer without relying on PyTorch's built-in `nn.Embedding`. \n",
    "The embedding layer maps token indices to dense vectors of fixed size. \n",
    "\n",
    "Key features:\n",
    "- Support for different initialization methods (uniform, normal, xavier)\n",
    "- Special handling for padding tokens\n",
    "- Manual implementation of the lookup process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([3, 8, 6])\n",
      "There are 3 sentences/batch size; 8 unique tokens; 6 d_model\n"
     ]
    }
   ],
   "source": [
    "class PyTorchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        token_size: int, \n",
    "        d_model: int, \n",
    "        padding_idx: Optional[int] = None,\n",
    "        init_method: str = 'uniform'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Custom embedding layer without using nn.Embedding\n",
    "        \n",
    "        Args:\n",
    "            token_size: Number of tokens in vocabulary\n",
    "            d_model: Embedding dimension\n",
    "            padding_idx: Index to set to zero\n",
    "            init_method: Weight initialization method\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Initialize weights based on method\n",
    "        if init_method == 'uniform':\n",
    "            self.weights = torch.rand(token_size, d_model) * 2 - 1  # [-1, 1]\n",
    "        elif init_method == 'normal':\n",
    "            self.weights = torch.randn(token_size, d_model)\n",
    "        elif init_method == 'xavier':\n",
    "            self.weights = torch.nn.init.xavier_uniform_(\n",
    "                torch.empty(token_size, d_model)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown init method: {init_method}\")\n",
    "        \n",
    "        # Zero out padding index if specified\n",
    "        if padding_idx is not None:\n",
    "            self.weights[padding_idx].zero_()\n",
    "        \n",
    "        self.token_size = token_size\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, token_sequences: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Lookup embeddings for given indices\n",
    "        \n",
    "        Args:\n",
    "            token_sequences: Tensor of token indices (# sentence, token_size)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of embedded tokens\n",
    "        \"\"\"\n",
    "        # Create output tensor\n",
    "        output = torch.zeros(\n",
    "            token_sequences.shape[0],  # batch size\n",
    "            token_sequences.shape[1],  # sequence length \n",
    "            self.d_model,      # embedding dimension\n",
    "            dtype=self.weights.dtype\n",
    "        )\n",
    "        print(f'Output shape: {output.shape}')\n",
    "        \n",
    "        # Manually lookup embeddings\n",
    "        for i, sentence in enumerate(token_sequences):\n",
    "            for j, token_idx in enumerate(sentence):\n",
    "                output[i, j] = self.weights[token_idx]\n",
    "        \n",
    "        return output\n",
    "    def __call__(self, indices: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Make the class callable for convenience\n",
    "        \"\"\"\n",
    "        return self.forward(indices)\n",
    "embedding = PyTorchEmbedding(token_size=20,\n",
    "                             d_model=6)\n",
    "outputs = embedding.forward(sequences)\n",
    "print((f'There are {outputs.shape[0]} sentences/batch size;'),\n",
    "    (f\"{outputs.shape[1]} unique tokens;\"),\n",
    "    f\"{outputs.shape[2]} d_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "\n",
    "Positional encoding is a crucial component of transformer architectures. Since transformers process tokens in parallel without any inherent notion of sequence order, we need to explicitly add position information.\n",
    "\n",
    "This implementation uses sinusoidal positional encoding as described in the \"Attention is All You Need\" paper:\n",
    "- Uses sine and cosine functions of different frequencies\n",
    "- Position information is added to the token embeddings\n",
    "- Each position is encoded as a unique vector that follows a specific pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_seq_length: int = 5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        position = torch.arange(max_seq_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Initialize the positional encoding buffer\n",
    "        pe = torch.zeros(1, max_seq_length, d_model)\n",
    "        \n",
    "        # Fill with sinusoidal pattern\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but part of the model state)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Add positional encoding to input embeddings\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size, seq_length, d_model]\n",
    "            \n",
    "        Returns:\n",
    "            Tensor with positional encoding added\n",
    "        \"\"\"\n",
    "        # Add positional encoding to input (only up to the sequence length)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention Mechanism\n",
    "\n",
    "Self-attention is the core component of transformer models that allows them to model dependencies between tokens regardless of their distance in the sequence.\n",
    "\n",
    "This implementation features:\n",
    "- Multi-head attention that splits the representation into multiple parts\n",
    "- Query, key, and value projections\n",
    "- Scaled dot-product attention with softmax normalization\n",
    "- Residual connection to preserve information flow\n",
    "\n",
    "The self-attention mechanism allows each position to attend to all positions in the sequence, capturing complex dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchSelfAttention(nn.Module):\n",
    "    def __init__(self, num_head: int = 8, d_model: int = 64):\n",
    "        super().__init__()\n",
    "        assert (d_model % num_head == 0), 'd_model must be divisible by num_head'\n",
    "        \n",
    "        self.num_head = num_head\n",
    "        self.d_model = d_model\n",
    "        self.d_head = int(d_model / num_head)\n",
    "        self.W_q = nn.Parameter(torch.randn(d_model, d_model))\n",
    "        self.W_v = nn.Parameter(torch.randn(d_model, d_model))\n",
    "        self.W_k = nn.Parameter(torch.randn(d_model, d_model))\n",
    "        self.W_o = nn.Parameter(torch.randn(d_model, d_model))\n",
    "\n",
    "    def split_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        return x.reshape(batch_size, seq_len, self.num_head, self.d_head).transpose(1, 2)\n",
    "    \n",
    "    def join_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[2]\n",
    "        return x.transpose(1, 2).reshape(batch_size, seq_len, self.d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for self-attention with optional masking.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, d_model)\n",
    "            mask: Mask tensor (batch_size, 1, seq_len, seq_len) or (1, 1, seq_len, seq_len)\n",
    "                  1s indicate positions to mask\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Input x: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Linear projections\n",
    "        q = torch.matmul(x, self.W_q)  # (batch_size, seq_len, d_model)\n",
    "        v = torch.matmul(x, self.W_v)  # (batch_size, seq_len, d_model)\n",
    "        k = torch.matmul(x, self.W_k)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Split heads\n",
    "        q = self.split_heads(q)  # Shape: (batch_size, num_heads, seq_len, d_head)\n",
    "        k = self.split_heads(k)  # Shape: (batch_size, num_heads, seq_len, d_head)\n",
    "        v = self.split_heads(v)  # Shape: (batch_size, num_heads, seq_len, d_head)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        # q.shape:   (batch_size, num_heads, seq_len, d_head)\n",
    "        # k.T.shape: (batch_size, num_heads, d_head, seq_len)\n",
    "        # Result:    (batch_size, num_heads, seq_len, seq_len)\n",
    "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            # Add a large negative value to masked positions to make their softmax score ~0\n",
    "            attention_scores = attention_scores.masked_fill(mask.bool(), -1e9)\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        # Maintains same shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Compute weighted sum of values\n",
    "        # attention_weights: (batch_size, num_heads, seq_len, seq_len)\n",
    "        # v:                 (batch_size, num_heads, seq_len, d_head)\n",
    "        # Result:            (batch_size, num_heads, seq_len, d_head)\n",
    "        head_output = torch.matmul(attention_weights, v)\n",
    "        \n",
    "        # Combine heads back to original dimension\n",
    "        output = self.join_heads(head_output)\n",
    "        \n",
    "        # Linear projection\n",
    "        output = torch.matmul(output, self.W_o)\n",
    "        \n",
    "        # Residual connection\n",
    "        output += x\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Config: batch_size=2, seq_len=10, d_model=64\n",
      "Input shape: torch.Size([2, 10, 64])\n",
      "Q after split shape: torch.Size([2, 8, 10, 8])\n",
      "Output shape: torch.Size([2, 10, 64])\n",
      "\n",
      "Config: batch_size=4, seq_len=20, d_model=128\n",
      "Input shape: torch.Size([4, 20, 128])\n",
      "Q after split shape: torch.Size([4, 8, 20, 16])\n",
      "Output shape: torch.Size([4, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "# Testing the self-attention implementation with different configurations\n",
    "test_configs = [\n",
    "        (2, 10, 64),   # batch_size=2, seq_len=10, d_model=64\n",
    "        (4, 20, 128),  # batch_size=4, seq_len=20, d_model=128\n",
    "    ]\n",
    "\n",
    "for batch_size, seq_len, d_model in test_configs:\n",
    "    # Create multi-head attention with default 8 heads\n",
    "    sa = PyTorchSelfAttention(num_head=8, d_model=d_model)\n",
    "    \n",
    "    # Create zero tensor input\n",
    "    x = torch.zeros(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Run forward pass\n",
    "    output = sa(x)\n",
    "    \n",
    "    print(f\"\\nConfig: batch_size={batch_size}, seq_len={seq_len}, d_model={d_model}\")\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    \n",
    "    # Verify intermediate shapes\n",
    "    q = torch.matmul(x, sa.W_q)\n",
    "    q_split = sa.split_heads(q)\n",
    "    print(\"Q after split shape:\", q_split.shape)\n",
    "    \n",
    "    # Verifying output shape matches input shape\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    assert output.shape == x.shape, \"Output shape must match input shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Block Implementation\n",
    "\n",
    "The encoder block combines several components of the transformer architecture:\n",
    "- Positional encoding to add position information\n",
    "- Multi-head self-attention to capture contextual relationships\n",
    "- Feed-forward neural network for further processing\n",
    "- Layer normalization and residual connections for stable training\n",
    "- Dropout for regularization\n",
    "\n",
    "This implementation includes two self-attention layers to increase the model's capacity to capture complex dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_head: int = 8, \n",
    "    d_ff: int = 2048, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = PyTorchSelfAttention(num_head=num_head, d_model=d_model)\n",
    "        self.self_attn2 = PyTorchSelfAttention(num_head=num_head, d_model=d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Self-attention block with residual connection and layer norm\n",
    "        attn_output = self.self_attn(x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward block with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        attn_output = self.self_attn2(x)\n",
    "        x = self.norm3(x + self.dropout(attn_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Config: batch_size=2, seq_len=10, d_model=64\n",
      "Input shape: torch.Size([2, 10, 64])\n",
      "Output shape: torch.Size([2, 10, 64])\n",
      "\n",
      "Config: batch_size=4, seq_len=20, d_model=128\n",
      "Input shape: torch.Size([4, 20, 128])\n",
      "Output shape: torch.Size([4, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "# Testing the encoder layer with different configurations\n",
    "test_configs = [\n",
    "        (2, 10, 64),   # batch_size=2, seq_len=10, d_model=64\n",
    "        (4, 20, 128),  # batch_size=4, seq_len=20, d_model=128\n",
    "    ]\n",
    "\n",
    "for batch_size, seq_len, d_model in test_configs:\n",
    "    # Create multi-head attention with default 8 heads\n",
    "    encoder = TransformerEncoderLayer(d_model=d_model,num_head=8)\n",
    "    \n",
    "    # Create zero tensor input\n",
    "    x = torch.zeros(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Run forward pass\n",
    "    output = encoder(x)\n",
    "    \n",
    "    print(f\"\\nConfig: batch_size={batch_size}, seq_len={seq_len}, d_model={d_model}\")\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    \n",
    "    # Verifying output shape matches input shape\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    assert output.shape == x.shape, \"Output shape must match input shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Decoder Layer\n",
    "\n",
    "The decoder layer is a crucial component in sequence-to-sequence transformer models. It processes the target sequence while attending to the encoder's output.\n",
    "\n",
    "Key components of the decoder layer:\n",
    "1. Masked self-attention - Allows each position to attend only to previous positions (causal masking)\n",
    "2. Cross-attention - Enables the decoder to attend to all positions in the encoder output\n",
    "3. Feed-forward network - Further processes the attended information\n",
    "4. Layer normalization and residual connections - Stabilize training\n",
    "\n",
    "The decoder produces outputs that can be used to predict the next token in an autoregressive fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_head: int = 8, d_ff: int = 2048, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Self-attention layer (with masking)\n",
    "        self.self_attn = PyTorchSelfAttention(num_head=num_head, d_model=d_model)\n",
    "        \n",
    "        # Cross-attention layer (to attend to encoder outputs)\n",
    "        self.cross_attn = PyTorchSelfAttention(num_head=num_head, d_model=d_model)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, \n",
    "                src_mask: Optional[torch.Tensor] = None, \n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Target sequence (batch_size, tgt_seq_len, d_model)\n",
    "            encoder_output: Output from encoder (batch_size, src_seq_len, d_model)\n",
    "            src_mask: Mask for encoder outputs (optional)\n",
    "            tgt_mask: Mask for decoder inputs (optional)\n",
    "        \"\"\"\n",
    "        # First sub-layer: Self-attention with causal mask\n",
    "        # Need to modify self_attn to handle the mask\n",
    "        attn_output = self.apply_attention(self.self_attn, x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Second sub-layer: Cross-attention to encoder outputs\n",
    "        cross_attn_output = self.apply_attention(self.cross_attn, x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        \n",
    "        # Third sub-layer: Feed-forward network\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def apply_attention(self, attention_layer: PyTorchSelfAttention, \n",
    "                        query: torch.Tensor, \n",
    "                        key: torch.Tensor, \n",
    "                        value: torch.Tensor, \n",
    "                        mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"Helper method to apply attention with masking\"\"\"\n",
    "        # This needs modifications to the original self-attention to handle masks\n",
    "        # For now, this is a placeholder assuming we'll extend PyTorchSelfAttention\n",
    "        return attention_layer(query)  # Would need to be modified to handle key, value, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Encoder Stack\n",
    "\n",
    "This class implements a full transformer encoder, which consists of:\n",
    "1. Positional encoding at the input\n",
    "2. A stack of N encoder layers\n",
    "3. A final layer normalization\n",
    "\n",
    "The encoder processes the source sequence and produces contextual representations that capture the relationships between tokens. These representations are then used by the decoder for generating the target sequence.\n",
    "\n",
    "Note: In a full transformer, the query and value come from the decoder while the key comes from the encoder in the cross-attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model: int, num_head: int, d_ff: int, \n",
    "                 num_layers: int, max_seq_length: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length)\n",
    "        \n",
    "        # Stack of encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_head, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final normalization layer\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Source sequence (batch_size, src_seq_len, d_model)\n",
    "            src_mask: Mask to avoid attending to padding tokens\n",
    "        \"\"\"\n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(src)\n",
    "        \n",
    "        # Apply each encoder layer in sequence\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)  # We'd need to modify this to pass the mask\n",
    "        \n",
    "        # Apply final normalization\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Decoder Stack\n",
    "\n",
    "The complete decoder stack consists of:\n",
    "1. Positional encoding for the target sequence\n",
    "2. Multiple decoder layers that process the target and attend to the encoder output\n",
    "3. A final layer normalization\n",
    "\n",
    "The decoder operates in an autoregressive manner during inference, predicting one token at a time. During training, teacher forcing is typically used where the entire target sequence is provided, but with appropriate masking to prevent attending to future tokens.\n",
    "\n",
    "The decoder combines information from the target sequence processed so far with the context provided by the encoder to generate predictions for the next tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model: int, num_head: int, d_ff: int, \n",
    "                 num_layers: int, max_seq_length: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length)\n",
    "        \n",
    "        # Stack of decoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, num_head, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final normalization layer\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, tgt: torch.Tensor, encoder_output: torch.Tensor,\n",
    "                src_mask: Optional[torch.Tensor] = None,\n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: Target sequence (batch_size, tgt_seq_len, d_model)\n",
    "            encoder_output: Output from encoder (batch_size, src_seq_len, d_model)\n",
    "            src_mask: Mask for encoder outputs\n",
    "            tgt_mask: Mask for decoder inputs\n",
    "        \"\"\"\n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(tgt)\n",
    "        \n",
    "        # Apply each decoder layer in sequence\n",
    "        for layer in self.layers:\n",
    "            # forward\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # Apply final normalization\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Transformer Model\n",
    "\n",
    "This class implements the full transformer architecture, combining:\n",
    "1. Input embeddings for source and target tokens\n",
    "2. The encoder stack to process the source sequence\n",
    "3. The decoder stack to generate the target sequence\n",
    "4. A final output projection to vocabulary size\n",
    "\n",
    "The transformer model is designed for sequence-to-sequence tasks like machine translation, text summarization, and question answering. It captures long-range dependencies effectively thanks to the self-attention mechanism.\n",
    "\n",
    "This implementation follows the architecture described in the \"Attention is All You Need\" paper, with customizable parameters for model size and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 d_model: int = 512, \n",
    "                 num_head: int = 8,\n",
    "                 d_ff: int = 2048,\n",
    "                 num_layers: int = 6,\n",
    "                 max_seq_length: int = 5000,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.src_embed = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embed = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # Encoder and decoder\n",
    "        self.encoder = TransformerEncoder(d_model, num_head, d_ff, num_layers, max_seq_length, dropout)\n",
    "        self.decoder = TransformerDecoder(d_model, num_head, d_ff, num_layers, max_seq_length, dropout)\n",
    "        \n",
    "        # Final output layer\n",
    "        self.output_layer = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "        \n",
    "    def _init_parameters(self):\n",
    "        \"\"\"Initialize parameters with Xavier uniform\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor,\n",
    "                src_mask: Optional[torch.Tensor] = None,\n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the entire transformer\n",
    "        \n",
    "        Args:\n",
    "            src: Source sequence (batch_size, src_seq_len)\n",
    "            tgt: Target sequence (batch_size, tgt_seq_len)\n",
    "            src_mask: Mask for source sequence\n",
    "            tgt_mask: Mask for target sequence\n",
    "            \n",
    "        Returns:\n",
    "            Output logits (batch_size, tgt_seq_len, tgt_vocab_size)\n",
    "        \"\"\"\n",
    "        # Embed source and target sequences\n",
    "        src_embedded = self.src_embed(src)\n",
    "        tgt_embedded = self.tgt_embed(tgt)\n",
    "        \n",
    "        # Encode the source sequence\n",
    "        encoder_output = self.encoder(src_embedded, src_mask)\n",
    "        \n",
    "        # Decode with encoder output and target sequence\n",
    "        decoder_output = self.decoder(tgt_embedded, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        output = self.output_layer(decoder_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking Mechanism\n",
    "\n",
    "Masking is essential in transformer models for two main purposes:\n",
    "\n",
    "1. **Padding Mask**: To prevent the model from attending to padding tokens in variable-length sequences.\n",
    "2. **Causal/Look-ahead Mask**: To ensure that predictions for a position can only depend on known outputs at previous positions (used in decoder).\n",
    "\n",
    "This implementation provides utility functions to create both types of masks and combine them when needed. Proper masking is crucial for both model performance and ensuring the autoregressive property during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq: torch.Tensor, pad_idx: int = 0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create mask for padding tokens\n",
    "    \n",
    "    Args:\n",
    "        seq: Input sequence (batch_size, seq_len)\n",
    "        pad_idx: Index representing padding token\n",
    "        \n",
    "    Returns:\n",
    "        Mask tensor (batch_size, 1, 1, seq_len)\n",
    "        1s indicate positions to mask (padding)\n",
    "    \"\"\"\n",
    "    # Create mask for padding: 1 for padding, 0 for non-padding\n",
    "    # Shape: (batch_size, seq_len)\n",
    "    mask = (seq == pad_idx).float()\n",
    "    \n",
    "    # Add dimensions for attention heads and query position\n",
    "    # Shape: (batch_size, 1, 1, seq_len)\n",
    "    return mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "def create_causal_mask(seq_len: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create causal mask to prevent attending to future positions\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Length of sequence\n",
    "        \n",
    "    Returns:\n",
    "        Mask tensor (1, 1, seq_len, seq_len)\n",
    "        1s indicate positions to mask (future positions)\n",
    "    \"\"\"\n",
    "    # Create a triangular matrix where 1s represent future positions to mask\n",
    "    # Shape: (seq_len, seq_len)\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).float()\n",
    "    \n",
    "    # Add batch and head dimensions\n",
    "    # Shape: (1, 1, seq_len, seq_len)\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Example usage:\n",
    "def get_masks(src: torch.Tensor, tgt: torch.Tensor, pad_idx: int = 0) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Get all masks needed for transformer training\n",
    "    \n",
    "    Args:\n",
    "        src: Source sequence (batch_size, src_seq_len)\n",
    "        tgt: Target sequence (batch_size, tgt_seq_len)\n",
    "        pad_idx: Index representing padding token\n",
    "        \n",
    "    Returns:\n",
    "        src_mask: Mask for source padding\n",
    "        tgt_mask: Combined mask for target (padding + causal)\n",
    "    \"\"\"\n",
    "    # Source padding mask\n",
    "    src_mask = create_padding_mask(src, pad_idx)\n",
    "    \n",
    "    # Target padding mask\n",
    "    tgt_padding_mask = create_padding_mask(tgt, pad_idx)\n",
    "    \n",
    "    # Target causal mask\n",
    "    tgt_seq_len = tgt.size(1)\n",
    "    tgt_causal_mask = create_causal_mask(tgt_seq_len)\n",
    "    \n",
    "    # Combine padding and causal masks for target\n",
    "    # We use broadcasting to combine the two masks\n",
    "    tgt_mask = torch.max(tgt_padding_mask, tgt_causal_mask)\n",
    "    \n",
    "    return src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Usage: Training the Transformer Model\n",
    "\n",
    "Here's sample code to train the transformer with 3 epochs. We'll use a small dataset for simplicity and quick training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/3, Loss: 2.5911\n",
      "Epoch 2/3, Loss: 2.3690\n",
      "Epoch 3/3, Loss: 2.1439\n",
      "Training completed!\n",
      "Input: deep learning is fascinating\n",
      "Translation: j'apprends\n"
     ]
    }
   ],
   "source": [
    "# Sample code to train transformer with 3 epochs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Sample parallel sentences (English -> French, very simplified example)\n",
    "src_sentences = [\n",
    "    \"hello world\",\n",
    "    \"how are you\",\n",
    "    \"I am learning transformers\",\n",
    "    \"this is interesting\",\n",
    "    \"machine learning is fun\",\n",
    "    \"deep learning models\",\n",
    "    \"natural language processing\",\n",
    "    \"attention is all you need\"\n",
    "]\n",
    "\n",
    "tgt_sentences = [\n",
    "    \"bonjour le monde\",\n",
    "    \"comment allez vous\",\n",
    "    \"j'apprends les transformers\",\n",
    "    \"c'est intéressant\",\n",
    "    \"l'apprentissage automatique est amusant\",\n",
    "    \"modèles d'apprentissage profond\",\n",
    "    \"traitement du langage naturel\",\n",
    "    \"l'attention est tout ce dont vous avez besoin\"\n",
    "]\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 64  # Small for demonstration\n",
    "num_heads = 4\n",
    "d_ff = 128\n",
    "num_layers = 2\n",
    "max_seq_length = 20\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "num_epochs = 3\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenizer_src = PyTorchTokenizer(max_vocab_size=100)\n",
    "tokenizer_tgt = PyTorchTokenizer(max_vocab_size=100)\n",
    "\n",
    "# Build vocabulary\n",
    "tokenizer_src.fit_on_texts(src_sentences)\n",
    "tokenizer_tgt.fit_on_texts(tgt_sentences)\n",
    "\n",
    "# Convert to sequences\n",
    "src_seq = tokenizer_src.texts_to_sequences(src_sentences, max_length=max_seq_length)\n",
    "tgt_seq = tokenizer_tgt.texts_to_sequences(tgt_sentences, max_length=max_seq_length)\n",
    "\n",
    "# For training, target input is the target sequence shifted right (removing the last token)\n",
    "# Target output is the target sequence shifted left (removing the first token, which is SOS)\n",
    "tgt_input = tgt_seq[:, :-1]\n",
    "tgt_output = tgt_seq[:, 1:]\n",
    "\n",
    "# Create masks\n",
    "src_mask, tgt_mask = get_masks(src_seq, tgt_input, pad_idx=tokenizer_src.PAD_IDX)\n",
    "\n",
    "# Create the transformer model\n",
    "model = Transformer(\n",
    "    src_vocab_size=tokenizer_src.vocab_size,\n",
    "    tgt_vocab_size=tokenizer_tgt.vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_head=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    num_layers=num_layers,\n",
    "    max_seq_length=max_seq_length\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer_tgt.PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Create mini-batches\n",
    "    num_samples = len(src_sentences)\n",
    "    indices = torch.randperm(num_samples)\n",
    "    \n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_indices = indices[i:i+batch_size]\n",
    "        \n",
    "        # Get batch data\n",
    "        src_batch = src_seq[batch_indices]\n",
    "        tgt_input_batch = tgt_input[batch_indices]\n",
    "        tgt_output_batch = tgt_output[batch_indices]\n",
    "        \n",
    "        # Get batch masks\n",
    "        src_mask_batch = src_mask[batch_indices] if src_mask is not None else None\n",
    "        tgt_mask_batch = tgt_mask[batch_indices] if tgt_mask is not None else None\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(src_batch, tgt_input_batch, src_mask_batch, tgt_mask_batch)\n",
    "        \n",
    "        # Reshape outputs and target for loss calculation\n",
    "        outputs = outputs.reshape(-1, outputs.shape[-1])\n",
    "        tgt_output_batch = tgt_output_batch.reshape(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, tgt_output_batch)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Print epoch statistics\n",
    "    avg_loss = total_loss / (len(src_sentences) // batch_size + 1)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Test translation\n",
    "def translate(model, src_text, tokenizer_src, tokenizer_tgt, max_length=50):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize source text\n",
    "    src = tokenizer_src.texts_to_sequences([src_text], max_length=max_seq_length)\n",
    "    src_mask = create_padding_mask(src, tokenizer_src.PAD_IDX)\n",
    "    \n",
    "    # Start with just the SOS token\n",
    "    tgt = torch.tensor([[tokenizer_tgt.SOS_IDX]])\n",
    "    \n",
    "    # Generate output token by token\n",
    "    for i in range(max_length):\n",
    "        # Create target mask\n",
    "        tgt_mask = create_causal_mask(tgt.size(1))\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(src, tgt, src_mask, tgt_mask)\n",
    "        \n",
    "        # Get next token prediction\n",
    "        next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
    "        \n",
    "        # Add predicted token to target sequence\n",
    "        tgt = torch.cat([tgt, next_token], dim=1)\n",
    "        \n",
    "        # Stop if EOS token is generated\n",
    "        if next_token.item() == tokenizer_tgt.EOS_IDX:\n",
    "            break\n",
    "    \n",
    "    # Convert ids to text\n",
    "    return tokenizer_tgt.sequences_to_texts(tgt)[0]\n",
    "\n",
    "# Test with a sample sentence\n",
    "test_sentence = \"deep learning is fascinating\"\n",
    "translation = translate(model, test_sentence, tokenizer_src, tokenizer_tgt)\n",
    "print(f\"Input: {test_sentence}\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Decoder Independently\n",
    "\n",
    "Here's code to run just the decoder part of the transformer, which can be useful for text generation tasks or when you want to implement a decoder-only architecture like GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.7190\n",
      "Epoch 2, Loss: 3.5732\n",
      "Epoch 3, Loss: 3.5058\n",
      "Epoch 4, Loss: 3.5054\n",
      "Epoch 5, Loss: 3.4779\n",
      "\n",
      "Prompt: the quick\n",
      "Generated: the quick quick picture actions quick question jumps a that day quick built to speak\n"
     ]
    }
   ],
   "source": [
    "# Code to run the decoder only\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, num_heads=8, \n",
    "                 d_ff=1024, num_layers=4, \n",
    "                 max_seq_length=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length)\n",
    "        \n",
    "        # Stack of decoder layers (but without cross-attention)\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # For masked attention\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Generate mask if not provided\n",
    "        if mask is None:\n",
    "            mask = create_causal_mask(x.size(1))\n",
    "            \n",
    "        # Embed tokens\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Apply each decoder layer (using self-attention only)\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x)  # Using encoder layer as our decoder-only layer\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        output = self.output_layer(x)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def generate(self, start_tokens, max_length=100, temperature=1.0):\n",
    "        self.eval()\n",
    "        \n",
    "        # Initialize with start tokens\n",
    "        current_tokens = start_tokens.clone()\n",
    "        \n",
    "        # Generate tokens auto-regressively\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass with current sequence\n",
    "            with torch.no_grad():\n",
    "                logits = self(current_tokens)\n",
    "            \n",
    "            # Get next token prediction (last position)\n",
    "            next_token_logits = logits[:, -1, :] / temperature\n",
    "            next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            current_tokens = torch.cat([current_tokens, next_token], dim=1)\n",
    "            \n",
    "            # Break if we reach a certain length\n",
    "            if current_tokens.size(1) >= self.max_seq_length:\n",
    "                break\n",
    "        \n",
    "        return current_tokens\n",
    "\n",
    "# Example usage of the decoder-only model\n",
    "def run_decoder_only_example():\n",
    "    # Simple text corpus\n",
    "    corpus = [\n",
    "        \"the quick brown fox jumps over the lazy dog\",\n",
    "        \"all that glitters is not gold\",\n",
    "        \"to be or not to be that is the question\",\n",
    "        \"a picture is worth a thousand words\",\n",
    "        \"actions speak louder than words\",\n",
    "        \"Rome wasn't built in a day\"\n",
    "    ]\n",
    "    \n",
    "    # Create and fit tokenizer\n",
    "    tokenizer = PyTorchTokenizer(max_vocab_size=100)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # Convert texts to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(corpus, max_length=20)\n",
    "    \n",
    "    # Create decoder-only model\n",
    "    model = DecoderOnlyTransformer(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        d_model=64,\n",
    "        num_heads=4,\n",
    "        d_ff=128,\n",
    "        num_layers=2,\n",
    "        max_seq_length=20\n",
    "    )\n",
    "    \n",
    "    # Train for a few steps (very basic for demonstration)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.PAD_IDX)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Quick training loop (just for demonstration)\n",
    "    model.train()\n",
    "    for epoch in range(5):  # Very few epochs for demonstration\n",
    "        total_loss = 0\n",
    "        \n",
    "        for seq in sequences:\n",
    "            # Create input and target (shifted by 1)\n",
    "            x = seq[:-1].unsqueeze(0)  # Input: all tokens except last, add batch dimension\n",
    "            y = seq[1:].unsqueeze(0)   # Target: all tokens except first, add batch dimension\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(x)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs.reshape(-1, outputs.shape[-1]), y.reshape(-1))\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(sequences):.4f}\")\n",
    "    \n",
    "    # Text generation with the trained model\n",
    "    prompt = \"the quick\"\n",
    "    prompt_tokens = tokenizer.texts_to_sequences([prompt], add_sos_eos=False)\n",
    "    \n",
    "    # Generate continuation\n",
    "    generated = model.generate(prompt_tokens, max_length=15, temperature=1.2)\n",
    "    generated_text = tokenizer.sequences_to_texts(generated)[0]\n",
    "    \n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "\n",
    "# Run the example\n",
    "run_decoder_only_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
