{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union, Tuple, Optional, Any\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import our custom tokenizer and embedding from previous notebook\n",
    "from utils.tokenizer import Tokenizer\n",
    "from utils.embedding_pc import Embedding,get_positional_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-do tokenization and embedding again to emphaisis it in my memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"sentence two\"\n",
    "]\n",
    "\n",
    "tokens = ''.join(texts).split() \n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit tokenizer on texts\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of senetence one in texts is (10, 64)\n",
      "The shape of the positional encoding is (10, 64)\n",
      "Dimension of after positional encoding is (10, 64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "d_model = 64  # Larger dimension for demonstration\n",
    "embedding = Embedding(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=d_model,\n",
    "    padding_idx=tokenizer.word_to_index[tokenizer.pad_token]\n",
    ")\n",
    "\n",
    "token_idxs = [tokenizer.word_to_index.get(word,\\\n",
    "                    tokenizer.word_to_index[tokenizer.unk_token]) \n",
    "              for word in tokens]\n",
    "token_embeddings = np.array([embedding(token_idx) for token_idx in token_idxs])\n",
    "# 9 words/9 tokens with 5 embedding dimension\n",
    "print(f'Dimension of senetence one in texts is {token_embeddings.shape}')\n",
    "\n",
    "# Generate positional encodings, aka noumber of tokens\n",
    "seq_length: int = len(token_embeddings)\n",
    "pos_encodings: np.ndarray = get_positional_encoding(seq_length, d_model=d_model)\n",
    "\n",
    "# Add positional encodings to token embeddings\n",
    "token_pos_embeddings: np.ndarray = token_embeddings + pos_encodings\n",
    "\n",
    "print(f'Dimension of after positional encoding is {token_pos_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logic\n",
    "## 1. Input sequence (Embedding of the tokens) perform matrix multiplication Q,v,K individually/.\n",
    "## 2. Split into multiple heads, d_model required to be divisible by num_heads\n",
    "## 3. Each head has dimension (seq_length,d_head) -> d_head == d_model/num_heads\n",
    "## 4. Compute attention weights for each head;\n",
    "## 5. attention weights are matrix muplification by Q * K.t, shape is (seq_len,seq_len)\n",
    "## 6. at mask to attnetion weights if set. add a large negative value (like -1e9) before softmax, which effectively results in near-zero attention after softmax is applied\n",
    "## 7. Softmax the attnetion weights to become 0 to 1, which mean relation/attention between each Q to each key\n",
    "## 8. attention weight * Value(seq_len,d_head) -> head's attention score (seq_len,d_head)\n",
    "## 9. Value in 2nd position since d_head need to return.\n",
    "## 10. Concat the list of attention scors to (seq_len,d_model)\n",
    "## 11. Linear projection by fully connected layer to (seq_len,d_model)\n",
    "## 12. Added residual to the output, residual is the embedding with positional enocoding\n",
    "## 13. output the attention weight for current attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-attention output shape: (9, 64)\n"
     ]
    }
   ],
   "source": [
    "# Create self-attention layer\n",
    "num_heads = 4\n",
    "attention = SelfAttention(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "# Run self-attention\n",
    "output, attention_weights = attention.forward(token_pos_embeddings)\n",
    "print(f\"Self-attention output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention:\n",
    "    def __init__(self, d_model: int, num_heads: int = 8):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "        assert (num_heads > 1), \"Number of heads should be greater than 1\"\n",
    "        assert (d_model % num_heads == 0), \"d_model must be divisible by num_heads\"\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        self.W_q = np.random.randn(d_model,d_model) * 0.1 #(d_model,d_dmodel)\n",
    "        self.W_k = np.random.randn(d_model, d_model) * 0.1  # Shape: (d_model, d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_model) * 0.1  # Shape: (d_model, d_model)\n",
    "        self.W_o = np.random.randn(d_model, d_model) * 0.1  # Shape: (d_model, d_model)\n",
    "    \n",
    "    def split_heads(self, x: np.array) ->np.array:\n",
    "        # x from (batch,seq_len, d_model) to (batch_size, num_head,seq_len,d_head)\n",
    "    \n",
    "        batch_size, seq_len , _ = x.shape\n",
    "        # reshape from (batch,seq_len, d_model) to (batch,seq_len, num_heads, d_heads)\n",
    "        x = x.reshape(batch_size,seq_len,self.num_heads,self.d_head)\n",
    "\n",
    "        return x.transpose(0,2,1,3)\n",
    "    def combine_head(self, x: np.array)->np.array:\n",
    "        # Combine all hteads back to d_model\n",
    "        # From (batch_size,num_heads,seq_len,d_head)\n",
    "        # to (batch_size,seq_len,d_model)\n",
    "    \n",
    "        batch_size , _ , seq_len , _ =x.shape\n",
    "        # (seq_len,num_head,d_head)\n",
    "        x = x.transpose(0,2,1,3)\n",
    "        return x.reshape(batch_size,seq_len,self.d_model)\n",
    "    def forward(self,x:np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Apply self-attention to input with residual connection.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        assert (len(x.shape)==3), 'Dimension of input need to be 3. (Batch_size,seq_len,d_model)'\n",
    "        residual = x.copy()\n",
    "        batch_size,seq_len,_ = x.shape\n",
    "\n",
    "        # linear projects\n",
    "        # (batch_size, seq_len, d_model) * (d_model,d_model)\n",
    "        q = np.dot(x,self.W_q)\n",
    "        k = np.dot(x,self.W_k)\n",
    "        v = np.dot(x,self.W_v)\n",
    "        # (batch_size, seq_len, d_model) \n",
    "        q = self.split_heads(q)  # Shape: (num_heads, seq_len, d_head)\n",
    "        k = self.split_heads(k)  # Shape: (num_heads, seq_len, d_head)\n",
    "        v = self.split_heads(v)  # Shape: (num_heads, seq_len, d_head)\n",
    "        \n",
    "        head_outputs = []\n",
    "        all_weights = []\n",
    "        for head_idx in range(self.num_heads):\n",
    "            head_q = q[head_idx] # one head (batch_size,seq_len,d_head)\n",
    "            head_k = k[head_idx]  # Shape: (batch_size,seq_len, d_head)\n",
    "            head_v = v[head_idx]  # Shape: (batch_size,seq_len, d_head)\n",
    "\n",
    "            # Attention score in current head\n",
    "            # Querys check with all keys\n",
    "            # (batch_size,seq_len,d_head)\n",
    "            scores = np.matmul(head_q, head_k.transpose(0, 2, 1)) / np.sqrt(self.d_head)\n",
    "\n",
    "            # apply softmax on d_head \n",
    "            weights = self._softmax(scores)\n",
    "            all_weights.append(weights)\n",
    "\n",
    "            head_output = np.dot(weights,head_v)\n",
    "\n",
    "            head_outputs.append(head_output)\n",
    "        # shape: (batch_size, num_heads, seq_len, d_head)\n",
    "        head_outputs = np.stack(head_outputs)\n",
    "        # shape: (batch_size, seq_len,d_mdoel)\n",
    "        output = self.combine_heads(head_outputs)\n",
    "        # Stack weights for returning\n",
    "        weights = np.stack(all_weights)\n",
    "        # Linear project\n",
    "        output = np.dot(output, self.W_o)  # Shape: (batch_size, seq_len, d_model)\n",
    "        output = output + residual  # Shape: (batch_size, seq_len, d_model)\n",
    "        return output, weights\n",
    "\n",
    "    def _softmax(self, x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "        x_max = np.max(x, axis=axis, keepdims=True)\n",
    "        e_x = np.exp(x - x_max)\n",
    "        return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Dimension of input need to be 3. (Batch_size,seq_len,d_model)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m attention = SelfAttention(d_model=d_model, num_heads=num_heads)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Run self-attention\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m output, attention_weights = \u001b[43mattention\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_pos_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSelf-attention output shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mSelfAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x:np.array) -> np.array:\n\u001b[32m     32\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03m    Apply self-attention to input with residual connection.\u001b[39;00m\n\u001b[32m     34\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m \u001b[33;03m        Output tensor of shape (batch_size, seq_len, d_model)\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(x.shape)==\u001b[32m3\u001b[39m), \u001b[33m'\u001b[39m\u001b[33mDimension of input need to be 3. (Batch_size,seq_len,d_model)\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     42\u001b[39m     residual = x.copy()\n\u001b[32m     43\u001b[39m     batch_size,seq_len,_ = x.shape\n",
      "\u001b[31mAssertionError\u001b[39m: Dimension of input need to be 3. (Batch_size,seq_len,d_model)"
     ]
    }
   ],
   "source": [
    "# Create self-attention layer\n",
    "num_heads = 4\n",
    "attention = SelfAttention(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "# Run self-attention\n",
    "output, attention_weights = attention.forward(token_pos_embeddings)\n",
    "print(f\"Self-attention output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
