{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union, Tuple, Optional, Any\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import our custom tokenizer and embedding from previous notebook\n",
    "from utils.tokenizer import Tokenizer\n",
    "from utils.embedding_pc import Embedding,get_positional_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-do tokenization and embedding again to emphaisis it in my memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of senetence one in texts is (9, 128)\n",
      "The shape of the positional encoding is (9, 128)\n",
      "Dimension of after positional encoding is (9, 128)\n"
     ]
    }
   ],
   "source": [
    "# Sample data for tokenizer\n",
    "texts = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "]\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit tokenizer on texts\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "d_model = 128  # Larger dimension for demonstration\n",
    "embedding = Embedding(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=d_model,\n",
    "    padding_idx=tokenizer.word_to_index[tokenizer.pad_token]\n",
    ")\n",
    "\n",
    "token_idxs = [tokenizer.word_to_index.get(word,\\\n",
    "                    tokenizer.word_to_index[tokenizer.unk_token]) \n",
    "              for word in texts[0].split()]\n",
    "token_embeddings = np.array([embedding(token_idx) for token_idx in token_idxs])\n",
    "# 9 words/9 tokens with 5 embedding dimension\n",
    "print(f'Dimension of senetence one in texts is {token_embeddings.shape}')\n",
    "\n",
    "# Generate positional encodings, aka noumber of tokens\n",
    "seq_length: int = len(token_embeddings)\n",
    "pos_encodings: np.ndarray = get_positional_encoding(seq_length, d_model=d_model)\n",
    "\n",
    "# Add positional encodings to token embeddings\n",
    "token_pos_embeddings: np.ndarray = token_embeddings + pos_encodings\n",
    "\n",
    "print(f'Dimension of after positional encoding is {token_pos_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SelfAttention:\n",
    "    def __init__(self, d_model: int, num_heads: int = 8):\n",
    "        \"\"\"\n",
    "        Initialize a self-attention layer.\n",
    "        \n",
    "        Args:\n",
    "            d_model: Dimensionality of the input embeddings\n",
    "            num_heads: Number of attention heads (defaults to 8)\n",
    "        \"\"\"\n",
    "        self.d_model = d_model  # Dimension of model embeddings\n",
    "        self.num_heads = num_heads  # Number of attention heads\n",
    "        self.d_head = d_model // num_heads if num_heads > 1 else d_model  # Dimension of each head\n",
    "        \n",
    "        # Initialize weights for Q, K, V projections\n",
    "        # For a basic implementation, we'll use random initialization\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        self.W_q = np.random.randn(d_model, d_model) * 0.1  # Shape: (d_model, d_model)\n",
    "        self.W_k = np.random.randn(d_model, d_model) * 0.1  # Shape: (d_model, d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_model) * 0.1  # Shape: (d_model, d_model)\n",
    "        self.W_o = np.random.randn(d_model, d_model) * 0.1  # Shape: (d_model, d_model)\n",
    "    \n",
    "    def split_heads(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, d_head)\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (seq_len, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape (num_heads, seq_len, d_head)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = 1, x.shape[0]  # Assuming no batch dimension for simplicity\n",
    "        \n",
    "        # Reshape to (seq_len, num_heads, d_head)\n",
    "        x = x.reshape(seq_len, self.num_heads, self.d_head)  # Shape: (seq_len, num_heads, d_head)\n",
    "        \n",
    "        # Transpose to (num_heads, seq_len, d_head)\n",
    "        return x.transpose(1, 0, 2)  # Shape: (num_heads, seq_len, d_head)\n",
    "    \n",
    "    def combine_heads(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Combine heads back to original shape\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (num_heads, seq_len, d_head)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape (seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Transpose to (seq_len, num_heads, d_head)\n",
    "        x = x.transpose(1, 0, 2)  # Shape: (seq_len, num_heads, d_head)\n",
    "        \n",
    "        # Combine last two dimensions\n",
    "        batch_size, seq_len = 1, x.shape[0]\n",
    "        return x.reshape(seq_len, self.d_model)  # Shape: (seq_len, d_model)\n",
    "    \n",
    "    def forward(self, x: np.ndarray, mask: np.ndarray = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply self-attention to input with residual connection.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (seq_len, d_model)\n",
    "            mask: Optional mask tensor of shape (seq_len, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (seq_len, d_model)\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[0]  # Sequence length\n",
    "        \n",
    "        # Store the input for the residual connection\n",
    "        residual = x  # Shape: (seq_len, d_model)\n",
    "        \n",
    "        # Linear projections\n",
    "        q = np.dot(x, self.W_q)  # Shape: (seq_len, d_model)\n",
    "        k = np.dot(x, self.W_k)  # Shape: (seq_len, d_model)\n",
    "        v = np.dot(x, self.W_v)  # Shape: (seq_len, d_model)\n",
    "        \n",
    "        if self.num_heads > 1:\n",
    "            # Split into heads\n",
    "            q = self.split_heads(q)  # Shape: (num_heads, seq_len, d_head)\n",
    "            k = self.split_heads(k)  # Shape: (num_heads, seq_len, d_head)\n",
    "            v = self.split_heads(v)  # Shape: (num_heads, seq_len, d_head)\n",
    "            \n",
    "            # Process each head independently\n",
    "            head_outputs = []\n",
    "            for head_idx in range(self.num_heads):\n",
    "                head_q = q[head_idx]  # Shape: (seq_len, d_head)\n",
    "                head_k = k[head_idx]  # Shape: (seq_len, d_head)\n",
    "                head_v = v[head_idx]  # Shape: (seq_len, d_head)\n",
    "                \n",
    "                # Compute attention scores\n",
    "                scores = np.dot(head_q, head_k.T)  # Shape: (seq_len, seq_len)\n",
    "                \n",
    "                # Scale attention scores\n",
    "                scores = scores / np.sqrt(self.d_head)  # Shape: (seq_len, seq_len)\n",
    "                \n",
    "                # Apply mask if provided; mask for causal attention when training\n",
    "                if mask is not None:\n",
    "                    scores = scores + (mask * -1e9)  # Shape: (seq_len, seq_len)\n",
    "                \n",
    "                # Apply softmax to get attention weights\n",
    "                weights = self._softmax(scores)  # Shape: (seq_len, seq_len)\n",
    "                \n",
    "                # Apply attention weights to values\n",
    "                head_output = np.dot(weights, head_v)  # Shape: (seq_len, d_head)\n",
    "                head_outputs.append(head_output)\n",
    "            \n",
    "            # Concatenate heads\n",
    "            output = np.concatenate(head_outputs, axis=-1)  # Shape: (seq_len, d_model)\n",
    "        else:\n",
    "            # Compute attention scores\n",
    "            scores = np.dot(q, k.T)  # Shape: (seq_len, seq_len)\n",
    "            \n",
    "            # Scale attention scores\n",
    "            scores = scores / np.sqrt(self.d_model)  # Shape: (seq_len, seq_len)\n",
    "            \n",
    "            # Apply mask if provided\n",
    "            if mask is not None:\n",
    "                scores = scores + (mask * -1e9)  # Shape: (seq_len, seq_len)\n",
    "            \n",
    "            # Apply softmax to get attention weights\n",
    "            weights = self._softmax(scores)  # Shape: (seq_len, seq_len)\n",
    "            \n",
    "            # Apply attention weights to values\n",
    "            output = np.dot(weights, v)  # Shape: (seq_len, d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = np.dot(output, self.W_o)  # Shape: (seq_len, d_model)\n",
    "        \n",
    "        # Apply residual connection: Add the input to the output\n",
    "        output = output + residual  # Shape: (seq_len, d_model)\n",
    "        \n",
    "        return output, weights  # Return attention weights for visualization\n",
    "    \n",
    "    def _softmax(self, x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute softmax values for each set of scores in x.\n",
    "        \n",
    "        Args:\n",
    "            x: Input array\n",
    "            axis: Axis along which to apply softmax\n",
    "            \n",
    "        Returns:\n",
    "            Softmax values\n",
    "        \"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        x_max = np.max(x, axis=axis, keepdims=True)\n",
    "        e_x = np.exp(x - x_max)\n",
    "        return e_x / np.sum(e_x, axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logic\n",
    "## 1. Input sequence (Embedding of the tokens) perform matrix multiplication Q,v,K individually/.\n",
    "## 2. Split into multiple heads, d_model required to be divisible by num_heads\n",
    "## 3. Each head has dimension (seq_length,d_head) -> d_head == d_model/num_heads\n",
    "## 4. Compute attention weights for each head;\n",
    "## 5. attention weights are matrix muplification by Q * K.t, shape is (seq_len,seq_len)\n",
    "## 6. at mask to attnetion weights if set. add a large negative value (like -1e9) before softmax, which effectively results in near-zero attention after softmax is applied\n",
    "## 7. Softmax the attnetion weights to become 0 to 1, which mean relation/attention between each Q to each key\n",
    "## 8. attention weight * Value(seq_len,d_head) -> head's attention score (seq_len,d_head)\n",
    "## 9. Value in 2nd position since d_head need to return.\n",
    "## 10. Concat the list of attention scors to (seq_len,d_model)\n",
    "## 11. Linear projection by fully connected layer to (seq_len,d_model)\n",
    "## 12. Added residual to the output, residual is the embedding with positional enocoding\n",
    "## 13. output the attention weight for current attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
